{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import F1Score\n",
    "import warnings\n",
    "import optuna\n",
    "import warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "FOLDS = 5 # number of folds for CV (== number of fusions tried)\n",
    "SEED = 2022"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# import data\n",
    "data = import_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocessing (tokenization, discard long sentence, lowercase etc.)\n",
    "data = preproc(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data split (CV)\n",
    "datasets = []\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "for i, (train_indices, test_indices) in enumerate(cv.split(data)):\n",
    "    train_set, test_set = data.loc[train_indices, :], data.loc[test_indices, :]\n",
    "\n",
    "    datasets.append((train_set, test_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4 # starting learning rate for scheduler\n",
    "EPOCHS = 'unrestricted' # use 'unrestricted' for full convergence\n",
    "N_LAYERS = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# template for training parent models (as we train them the same way)\n",
    "def train_early_stopping(model_name: str, train_iter, valid_iter, embedding, pad_idx, voc_size, device, lr=2e-4, save=True):\n",
    "    # init\n",
    "    model = new_model(embedding, pad_idx, voc_size, device, n_layers=N_LAYERS) # init model\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train with early stopping\n",
    "    history = train(model=model,\n",
    "                    iterator=train_iter,\n",
    "                    valid_iter=valid_iter,\n",
    "                    optimizer=opt,\n",
    "                    criterion=loss_fn,\n",
    "                    epoch=EPOCHS,\n",
    "                    clip=1,\n",
    "                    device=device)\n",
    "\n",
    "    if save:\n",
    "        # save model\n",
    "        name = f'parallel_training/model{model_name}_IMDB_256'\n",
    "        save_model(model, name=name)\n",
    "\n",
    "        # save history\n",
    "        name = f'parallel_training/history_model{model_name}_IMDB_256'\n",
    "        save_history(history, name=name)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model_names = ['A', 'B', 'vanilla_pre', 'vanilla_post', 'OT_pre (method 1)', 'OT_post (method 1)', 'OT_pre (method 2)', 'OT_post (method 2)', 'OT_pre (method 3)', 'OT_post (method 3)', 'random']\n",
    "scores = {'loss': {model_name: [] for model_name in model_names},\n",
    "          'accuracy': {model_name: [] for model_name in model_names},\n",
    "          'f1': {model_name: [] for model_name in model_names},}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:10\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\utils.py:71\u001B[0m, in \u001B[0;36mbuild_generators\u001B[1;34m(train, test, device, batch_size)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m item:\n\u001B[0;32m     70\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m vocab:\n\u001B[1;32m---> 71\u001B[0m             vocab[word] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(vocab)\n\u001B[0;32m     72\u001B[0m pad_idx \u001B[38;5;241m=\u001B[39m vocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__PAD__\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     73\u001B[0m voc_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(vocab)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(FOLDS):\n",
    "        print(f'Fold {i + 1}/{FOLDS}')\n",
    "\n",
    "        # get training, test set\n",
    "        train_set, test_set = datasets.pop()\n",
    "\n",
    "        # build generators\n",
    "        train_iterator, test_iterator, voc_size, pad_idx, embedding = build_generators(train_set, test_set, device, batch_size=512)\n",
    "\n",
    "        # train parent models\n",
    "        train_parent = lambda x: train_early_stopping(model_name=x,\n",
    "                                                      train_iter=train_iterator,\n",
    "                                                      valid_iter=test_iterator,\n",
    "                                                      embedding=embedding,\n",
    "                                                      pad_idx=pad_idx,\n",
    "                                                      voc_size=voc_size,\n",
    "                                                      device=device,\n",
    "                                                      lr=LEARNING_RATE,\n",
    "                                                      save=False)\n",
    "        print('Starting training for model A')\n",
    "        modelA = train_parent('A')\n",
    "        print('Starting training for model B')\n",
    "        modelB = train_parent('B')\n",
    "\n",
    "        # model fusion\n",
    "        # 1) vanilla\n",
    "        model_fusion_vanilla = vanilla_fusion(modelA=modelA, modelB=modelB, pad_idx=pad_idx, voc_size=voc_size, embedding=embedding, device=device)\n",
    "        # 2) optimal transport\n",
    "        N_TRIALS = 50\n",
    "        variations = ['multilayer', 'multihead-multilayer']\n",
    "        # 2.1) method 1\n",
    "        variation = variations[0]\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(weighted_fusion(modelA, modelB, train_iterator, test_iterator, embedding, pad_idx, voc_size, device, variation=variation), n_trials=N_TRIALS)\n",
    "        best_weighting_factor = study.best_params['weighting_factor']\n",
    "        print('Best fusion weight (method 1):', best_weighting_factor)\n",
    "        model_fusion_1 = ot_fusion(modelA, modelB, train_iterator, embedding, pad_idx, voc_size, device, fusion_ratio=best_weighting_factor, variation=variation)\n",
    "        # 2.2) method 2\n",
    "        variation = variations[1]\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(weighted_fusion(modelA, modelB, train_iterator, test_iterator, embedding, pad_idx, voc_size, device, variation=variation), n_trials=N_TRIALS)\n",
    "        best_weighting_factor = study.best_params['weighting_factor']\n",
    "        print('Best fusion weight (method 2):', best_weighting_factor)\n",
    "        model_fusion_2 = ot_fusion(modelA, modelB, train_iterator, embedding, pad_idx, voc_size, device, fusion_ratio=best_weighting_factor, variation=variation)\n",
    "\n",
    "        # evaluate\n",
    "        # ensure all models on same device\n",
    "        model_to_cpu = lambda x: x.to(device)\n",
    "        modelA = model_to_cpu(modelA)\n",
    "        modelB = model_to_cpu(modelB)\n",
    "        model_random = new_model(embedding, pad_idx, voc_size, device, n_layers=N_LAYERS)\n",
    "        model_fusion_1 = model_to_cpu(model_fusion_1)\n",
    "        model_fusion_2 = model_to_cpu(model_fusion_2)\n",
    "        model_fusion_3 = model_to_cpu(model_fusion_3)\n",
    "        model_fusion_vanilla = model_to_cpu(model_fusion_vanilla)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('A', 'B', 'random', 'OT_pre (method 1)', 'OT_pre (method 2)', 'vanilla_pre'),\n",
    "                               (modelA, modelB, model_random, model_fusion_1, model_fusion_2, model_fusion_vanilla)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)\n",
    "\n",
    "        # retraining\n",
    "        retrain = lambda x: train(model=x,\n",
    "                                  iterator=train_iterator,\n",
    "                                  valid_iter=test_iterator,\n",
    "                                  optimizer=torch.optim.SGD(x.parameters(), lr=LEARNING_RATE),\n",
    "                                  criterion=nn.CrossEntropyLoss(),\n",
    "                                  epoch=EPOCHS,\n",
    "                                  clip=1,\n",
    "                                  device=device)\n",
    "        # 1) vanilla\n",
    "        # train with early stopping\n",
    "        print('Starting retraining for model vanilla fusion')\n",
    "        retrain(model_fusion_vanilla)\n",
    "\n",
    "        # 2) optimal transport\n",
    "        # train with early stopping\n",
    "        print('Starting retraining for model OT fusion (method 1)')\n",
    "        retrain(model_fusion_1)\n",
    "        print('Starting retraining for model OT fusion (method 2)')\n",
    "        retrain(model_fusion_2)\n",
    "\n",
    "        # evaluate\n",
    "        # ensure all models on same device\n",
    "        model_fusion_1 = model_to_cpu(model_fusion_1)\n",
    "        model_fusion_2 = model_to_cpu(model_fusion_2)\n",
    "        model_fusion_vanilla = model_to_cpu(model_fusion_vanilla)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('OT_post (method 1)', 'OT_post (method 2)', 'vanilla_post'), (model_fusion_1, model_fusion_2, model_fusion_vanilla)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export as LaTeX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model_names_latex = ['Model A', 'Model B', 'Vanilla', 'Vanilla (retraining)', 'Optimal transport (method 1)', 'Optimal transport (method 1 - retraining)',\n",
    "                     'Optimal transport (method 2)', 'Optimal transport (method 2 - retraining)', 'Optimal transport (method 3)', 'Optimal transport (method 3 - retraining)', 'Untrained model (baseline)']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   loss accuracy  f1\n",
      "A                    []       []  []\n",
      "B                    []       []  []\n",
      "vanilla_pre          []       []  []\n",
      "vanilla_post         []       []  []\n",
      "OT_pre (method 1)    []       []  []\n",
      "OT_post (method 1)   []       []  []\n",
      "OT_pre (method 2)    []       []  []\n",
      "OT_post (method 2)   []       []  []\n",
      "OT_pre (method 3)    []       []  []\n",
      "OT_post (method 3)   []       []  []\n",
      "random               []       []  []\n",
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\caption{Model performance (5-fold CV)}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &             Loss &         Accuracy &         F1 score \\\\\n",
      "\\midrule\n",
      "\\textbf{Model A                                  } &  \\textbf{nan ± nan} &  \\textbf{nan ± nan} &  \\textbf{nan ± nan} \\\\\n",
      "\\textbf{Model B                                  } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Vanilla                                  } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Vanilla (retraining)                     } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 1)             } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 1 - retraining)} &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 2)             } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 2 - retraining)} &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 3)             } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (method 3 - retraining)} &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Untrained model (baseline)               } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\jjung\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\utils.py:1007: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex = df.to_latex(index=True,\n"
     ]
    }
   ],
   "source": [
    "latex = scores_to_latex(scores, model_names_latex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# save as latex (in text format) (optional)\n",
    "with open('./Output/scores_different_seeds_multilayer.txt','w') as dat:\n",
    "    dat.write(str(latex))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}