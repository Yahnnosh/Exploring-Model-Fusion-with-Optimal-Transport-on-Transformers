{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import F1Score\n",
    "import optuna\n",
    "import warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "FOLDS = 5 # number of folds for CV (== number of fusions tried)\n",
    "SEED = 2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# import data\n",
    "data = import_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the data...\n",
      "Length of the data :  29544\n",
      "CPU times: total: 3min 55s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocessing (tokenization, discard long sentence, lowercase etc.)\n",
    "data = preproc(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 17.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data split (CV)\n",
    "datasets = []\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "for i, (train_indices, test_indices) in enumerate(cv.split(data)):\n",
    "    train_set, test_set = data.loc[train_indices, :], data.loc[test_indices, :]\n",
    "\n",
    "    datasets.append((train_set, test_set))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-3 # starting learning rate for scheduler\n",
    "EPOCHS = 40 # use 'unrestricted' for full convergence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# template for training parent models (as we train them the same way)\n",
    "def train_early_stopping(model_name: str, train_iter, valid_iter, embedding, pad_idx, voc_size, device, epochs='unrestricted', lr=2e-4, save=True):\n",
    "    # init\n",
    "    model = new_model(embedding, pad_idx, voc_size, device) # init model\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train with early stopping\n",
    "    '''history = train(model=model,\n",
    "                    iterator=train_iter,\n",
    "                    valid_iter=valid_iter,\n",
    "                    optimizer=opt,\n",
    "                    criterion=loss_fn,\n",
    "                    epoch=epochs,\n",
    "                    clip=1,\n",
    "                    device=device)'''\n",
    "\n",
    "    history, best_model, best_model_score = train_save_best(model=model,\n",
    "                                                               iterator=train_iter,\n",
    "                                                               valid_iter=valid_iter,\n",
    "                                                               optimizer=opt,\n",
    "                                                               criterion=loss_fn,\n",
    "                                                               epoch=epochs,\n",
    "                                                               clip=1,\n",
    "                                                               device=device)\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    if save:\n",
    "        # save model\n",
    "        name = f'parallel_training/model{model_name}_IMDB_256'\n",
    "        save_model(model, name=name)\n",
    "\n",
    "        # save history\n",
    "        name = f'parallel_training/history_model{model_name}_IMDB_256'\n",
    "        save_history(history, name=name)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_names = ['A', 'B', 'OT_pre', 'OT_calibr', 'OT_last_layer', 'random']\n",
    "scores = {'loss': {model_name: [] for model_name in model_names},\n",
    "          'accuracy': {model_name: [] for model_name in model_names},\n",
    "          'f1': {model_name: [] for model_name in model_names},}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(FOLDS):\n",
    "        print(f'Fold {i + 1}/{FOLDS}')\n",
    "\n",
    "        # get training, test set\n",
    "        train_set, test_set = datasets.pop()\n",
    "\n",
    "        # build generators\n",
    "        train_iterator, test_iterator, voc_size, pad_idx, embedding = build_generators(train_set, test_set, device, batch_size=512)\n",
    "\n",
    "        # train parent models\n",
    "        train_parent = lambda x: train_early_stopping(model_name='A',\n",
    "                                                      train_iter=train_iterator,\n",
    "                                                      valid_iter=test_iterator,\n",
    "                                                      embedding=embedding,\n",
    "                                                      pad_idx=pad_idx,\n",
    "                                                      voc_size=voc_size,\n",
    "                                                      device=device,\n",
    "                                                      epochs=EPOCHS,\n",
    "                                                      lr=LEARNING_RATE,\n",
    "                                                      save=False)\n",
    "        print('Starting training for model A')\n",
    "        modelA = train_parent('A')\n",
    "        print('Starting training for model B')\n",
    "        modelB = train_parent('B')\n",
    "\n",
    "        # model fusion\n",
    "        # optimal transport\n",
    "        N_TRIALS = 50\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(weighted_fusion(modelA, modelB, train_iterator, test_iterator, embedding, pad_idx, voc_size, device), n_trials=N_TRIALS)\n",
    "        best_weighting_factor = study.best_params['weighting_factor']\n",
    "        print('Best fusion weight:', best_weighting_factor)\n",
    "        model_fusion = ot_fusion(modelA, modelB, train_iterator, embedding, pad_idx, voc_size, device, fusion_ratio=best_weighting_factor, drop_prob=0.2)\n",
    "\n",
    "        # evaluate\n",
    "        # ensure all models on same device\n",
    "        model_to_cpu = lambda x: x.to(device)\n",
    "        modelA = model_to_cpu(modelA)\n",
    "        modelB = model_to_cpu(modelB)\n",
    "        model_random = new_model(embedding, pad_idx, voc_size, device)\n",
    "        model_fusion = model_to_cpu(model_fusion)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('A', 'B', 'random', 'OT_pre'), (modelA, modelB, model_random, model_fusion)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)\n",
    "\n",
    "        # recalibration\n",
    "        calibrate_norm_layer(model_fusion, train_iterator, device, learning_rate=0.01)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('OT_calibr'), (model_fusion)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)\n",
    "\n",
    "        # retraining last layer\n",
    "        train_last_layer(model_fusion, train_iterator, device, learning_rate=1e-3)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('OT_last_layer'), (model_fusion)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export as LaTeX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_names_latex = ['Model A', 'Model B', 'OT + weighted fusion', 'OT + weighted fusion (recalibrated)','OT + weighted fusion (last layer retrained)', 'Untrained model (baseline)']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latex = scores_to_latex(scores, model_names_latex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# save as latex (in text format) (optional)\n",
    "with open('./Output/scores_different_seeds.txt','w') as dat:\n",
    "    dat.write(str(latex))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}