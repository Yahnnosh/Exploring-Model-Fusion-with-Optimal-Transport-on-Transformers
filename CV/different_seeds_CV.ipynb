{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import F1Score\n",
    "import optuna\n",
    "import warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "FOLDS = 5 # number of folds for CV (== number of fusions tried)\n",
    "SEED = 2022"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# import data\n",
    "data = import_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the data...\n",
      "Length of the data :  29544\n",
      "CPU times: total: 4min 7s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocessing (tokenization, discard long sentence, lowercase etc.)\n",
    "data = preproc(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 29.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data split (CV)\n",
    "datasets = []\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "for i, (train_indices, test_indices) in enumerate(cv.split(data)):\n",
    "    train_set, test_set = data.loc[train_indices, :], data.loc[test_indices, :]\n",
    "\n",
    "    datasets.append((train_set, test_set))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4 # starting learning rate for scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# template for training parent models (as we train them the same way)\n",
    "def train_early_stopping(model_name: str, train_iter, valid_iter, embedding, pad_idx, voc_size, device, lr=2e-4, save=True):\n",
    "    # init\n",
    "    model = new_model(embedding, pad_idx, voc_size, device) # init model\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train with early stopping\n",
    "    history = train(model=model,\n",
    "                    iterator=train_iter,\n",
    "                    valid_iter=valid_iter,\n",
    "                    optimizer=opt,\n",
    "                    criterion=loss_fn,\n",
    "                    epoch='unrestricted',\n",
    "                    clip=1,\n",
    "                    device=device)\n",
    "\n",
    "    if save:\n",
    "        # save model\n",
    "        name = f'parallel_training/model{model_name}_IMDB_256'\n",
    "        save_model(model, name=name)\n",
    "\n",
    "        # save history\n",
    "        name = f'parallel_training/history_model{model_name}_IMDB_256'\n",
    "        save_history(history, name=name)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model_names = ['A', 'B', 'vanilla_pre', 'vanilla_post', 'OT_pre', 'OT_post', 'random']\n",
    "scores = {'loss': {model_name: [] for model_name in model_names},\n",
    "          'accuracy': {model_name: [] for model_name in model_names},\n",
    "          'f1': {model_name: [] for model_name in model_names},}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Vocabulary Size :  23044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23636/23636 [00:01<00:00, 18523.76it/s]\n",
      "100%|██████████| 5908/5908 [00:00<00:00, 16411.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive labels ratio (train set): 0.5019461837874429\n",
      "Positive labels ratio (test set): 0.506093432633717\n",
      "Dataset initializing done\n",
      "Starting training for model A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 33 - Train Loss: 0.3387 / Validation Loss: 0.4903 / Train acc: 0.8544 / Val acc: 0.7817 / Learning Rate : 0.0002:   0%|          | 33/1000000 [01:28<744:10:02,  2.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:23\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "File \u001B[1;32m<timed exec>:13\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mtrain_early_stopping\u001B[1;34m(model_name, train_iter, valid_iter, embedding, pad_idx, voc_size, device, lr, save)\u001B[0m\n\u001B[0;32m      7\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# train with early stopping\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                \u001B[49m\u001B[43miterator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                \u001B[49m\u001B[43mvalid_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43munrestricted\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                \u001B[49m\u001B[43mclip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m save:\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# save model\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparallel_training/model\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_IMDB_256\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\utils.py:314\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, iterator, valid_iter, optimizer, criterion, epoch, clip, device, termination_criterion)\u001B[0m\n\u001B[0;32m    312\u001B[0m src, trg \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(src)\u001B[38;5;241m.\u001B[39mto(device), torch\u001B[38;5;241m.\u001B[39mtensor(trg)\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# put to cpu/gpu\u001B[39;00m\n\u001B[0;32m    313\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# reset optimizer\u001B[39;00m\n\u001B[1;32m--> 314\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# predict\u001B[39;00m\n\u001B[0;32m    315\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(output, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# logits -> labels\u001B[39;00m\n\u001B[0;32m    316\u001B[0m output_reshape \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, output\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\transformer.py:445\u001B[0m, in \u001B[0;36mTransformerClassifier.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    444\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_pad_mask(x, x)\n\u001B[1;32m--> 445\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    447\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\transformer.py:266\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x, src_mask)\u001B[0m\n\u001B[0;32m    263\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb(x)\n\u001B[0;32m    265\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 266\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\transformer.py:229\u001B[0m, in \u001B[0;36mEncoderLayer.forward\u001B[1;34m(self, x, src_mask)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, src_mask):\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;66;03m# 1. compute self attention\u001B[39;00m\n\u001B[0;32m    228\u001B[0m     _x \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m--> 229\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;66;03m# 2. add and norm\u001B[39;00m\n\u001B[0;32m    232\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m _x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\transformer.py:111\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[1;34m(self, q, k, v, mask)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# 4. concat and pass to linear layer\u001B[39;00m\n\u001B[0;32m    110\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconcat(out)\n\u001B[1;32m--> 111\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# 5. visualize attention map\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;66;03m# TODO : we should implement visualization\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(FOLDS):\n",
    "        print(f'Fold {i + 1}/{FOLDS}')\n",
    "\n",
    "        # get training, test set\n",
    "        train_set, test_set = datasets.pop()\n",
    "\n",
    "        # build generators\n",
    "        train_iterator, test_iterator, voc_size, pad_idx, embedding = build_generators(train_set, test_set, device, batch_size=512)\n",
    "\n",
    "        # train parent models\n",
    "        train_parent = lambda x: train_early_stopping(model_name=x,\n",
    "                                                      train_iter=train_iterator,\n",
    "                                                      valid_iter=test_iterator,\n",
    "                                                      embedding=embedding,\n",
    "                                                      pad_idx=pad_idx,\n",
    "                                                      voc_size=voc_size,\n",
    "                                                      device=device,\n",
    "                                                      lr=LEARNING_RATE,\n",
    "                                                      save=False)\n",
    "        print('Starting training for model A')\n",
    "        modelA = train_parent('A')\n",
    "        print('Starting training for model B')\n",
    "        modelB = train_parent('B')\n",
    "\n",
    "        # model fusion\n",
    "        # 1) vanilla\n",
    "        model_fusion_vanilla = vanilla_fusion(modelA=modelA, modelB=modelB, pad_idx=pad_idx, voc_size=voc_size, embedding=embedding, device=device)\n",
    "        # 2) optimal transport\n",
    "        N_TRIALS = 100\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(weighted_fusion(modelA, modelB, train_iterator, test_iterator, embedding, pad_idx, voc_size, device), n_trials=N_TRIALS)\n",
    "        best_weighting_factor = study.best_params['weighting_factor']\n",
    "        print('Best fusion weight:', best_weighting_factor)\n",
    "        model_fusion = ot_fusion(modelA, modelB, train_iterator, embedding, pad_idx, voc_size, device, fusion_ratio=best_weighting_factor)\n",
    "\n",
    "        # evaluate\n",
    "        # ensure all models on same device\n",
    "        model_to_cpu = lambda x: x.to(device)\n",
    "        modelA = model_to_cpu(modelA)\n",
    "        modelB = model_to_cpu(modelB)\n",
    "        model_random = new_model(embedding, pad_idx, voc_size, device)\n",
    "        model_fusion = model_to_cpu(model_fusion)\n",
    "        model_fusion_vanilla = model_to_cpu(model_fusion_vanilla)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('A', 'B', 'random', 'OT_pre', 'vanilla_pre'), (modelA, modelB, model_random, model_fusion, model_fusion_vanilla)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)\n",
    "\n",
    "        # retraining\n",
    "        retrain = lambda x: train(model=x,\n",
    "                                  iterator=train_iterator,\n",
    "                                  valid_iter=test_iterator,\n",
    "                                  optimizer=torch.optim.SGD(x.parameters(), lr=LEARNING_RATE),\n",
    "                                  criterion=nn.CrossEntropyLoss(),\n",
    "                                  epoch='unrestricted',\n",
    "                                  clip=1,\n",
    "                                  device=device)\n",
    "        # 1) vanilla\n",
    "        # train with early stopping\n",
    "        print('Starting retraining for model vanilla fusion')\n",
    "        retrain(model_fusion_vanilla)\n",
    "\n",
    "        # 2) optimal transport\n",
    "        # train with early stopping\n",
    "        print('Starting retraining for model OT fusion')\n",
    "        retrain(model_fusion)\n",
    "\n",
    "        # evaluate\n",
    "        # ensure all models on same device\n",
    "        model_fusion = model_to_cpu(model_fusion)\n",
    "        model_fusion_vanilla = model_to_cpu(model_fusion_vanilla)\n",
    "\n",
    "        # test models\n",
    "        for name, model in zip(('OT_post', 'vanilla_post'), (model_fusion, model_fusion_vanilla)):\n",
    "            loss, acc, f1 = validation(model, test_iterator, nn.CrossEntropyLoss(), device) # (loss, accuracy, f1)\n",
    "\n",
    "            # put into cpu\n",
    "            to_cpu = lambda x: x.to('cpu') if isinstance(x, torch.Tensor) else x\n",
    "            loss = to_cpu(loss)\n",
    "            acc = to_cpu(acc)\n",
    "            f1 = to_cpu(f1)\n",
    "\n",
    "            print(name, f'loss: {loss} - accuracy: {acc} - f1: {f1}')\n",
    "            scores['loss'][name].append(loss), scores['accuracy'][name].append(acc), scores['f1'][name].append(f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export as LaTeX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model_names_latex = ['Model A', 'Model B', 'Vanilla', 'Vanilla (retraining)', 'Optimal transport', 'Optimal transport (retraining)', 'Untrained model (baseline)']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             loss accuracy  f1\n",
      "A              []       []  []\n",
      "B              []       []  []\n",
      "vanilla_pre    []       []  []\n",
      "vanilla_post   []       []  []\n",
      "OT_pre         []       []  []\n",
      "OT_post        []       []  []\n",
      "random         []       []  []\n",
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\caption{Model performance (5-fold CV)}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &             Loss &         Accuracy &         F1 score \\\\\n",
      "\\midrule\n",
      "\\textbf{Model A                       } &  \\textbf{nan ± nan} &  \\textbf{nan ± nan} &  \\textbf{nan ± nan} \\\\\n",
      "\\textbf{Model B                       } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Vanilla                       } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Vanilla (retraining)          } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport             } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Optimal transport (retraining)} &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\textbf{Untrained model (baseline)    } &        nan ± nan &        nan ± nan &        nan ± nan \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\jjung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\jjung\\OneDrive\\Dokumente\\ETH\\MSc 3rd semester\\Deep Learning\\DL Project\\CV\\utils.py:1010: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex = df.to_latex(index=True,\n"
     ]
    }
   ],
   "source": [
    "latex = scores_to_latex(scores, model_names_latex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# save as latex (in text format) (optional)\n",
    "with open('./Output/scores_different_seeds (weighted full retraining).txt','w') as dat:\n",
    "    dat.write(str(latex))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}