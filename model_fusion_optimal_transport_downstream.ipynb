{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dataloader import *\n",
    "from transformer import *\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import ot\n",
    "import torch\n",
    "import optuna\n",
    "from torchmetrics.classification import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, iterator, criterion, device):\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # validation\n",
    "    # loss, metrics for current epoch\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_accuracy = 0\n",
    "\n",
    "    labels_val = []\n",
    "    preds_val = []\n",
    "    f1_scorer = F1Score(task='binary').to(device)\n",
    "\n",
    "    with torch.no_grad(): # stop graph\n",
    "        # batches\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            output = model(src)\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "\n",
    "            labels_val.append(trg)\n",
    "            preds_val.append(y_pred)\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_accuracy += accuracy\n",
    "\n",
    "    # put to numpy\n",
    "    labels_val = torch.cat(labels_val)\n",
    "    preds_val = torch.cat(preds_val)\n",
    "    # return mean loss w.r.t. batches\n",
    "    return val_epoch_loss / len(iterator), val_epoch_accuracy / len(iterator), f1_scorer(preds_val, labels_val)\n",
    "\n",
    "def plot_training(history, marker=None):\n",
    "    # put everything on cpu\n",
    "    for key, value in history.items():\n",
    "        history[key] = [element.cpu() if isinstance(element, torch.Tensor) else element for element in value]\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.01,\n",
    "                    right=1.5,\n",
    "                    top=0.6,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title('Training loss')\n",
    "\n",
    "    # vertical line for marking best epoch\n",
    "    if marker is not None:\n",
    "        y_min = min(history['train_loss'] + history['val_loss'])\n",
    "        y_max = max(history['train_loss'] + history['val_loss'])\n",
    "        plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title('Training metric')\n",
    "\n",
    "    # vertical line for marking best epoch\n",
    "    if marker is not None:\n",
    "        y_min = min(history['train_acc'] + history['val_acc'])\n",
    "        y_max = max(history['train_acc'] + history['val_acc'])\n",
    "        plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def train_save_best(model, iterator, valid_iter, optimizer, criterion, epoch, clip, device):\n",
    "\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': [],\n",
    "               'learning_rate': []}\n",
    "    best_model = None\n",
    "    best_model_score = 1e9\n",
    "    best_model_epoch = 0\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc, val_f1 = validation(model, valid_iter, optimizer, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc, current_lr]):\n",
    "                history[key].append(value)\n",
    "\n",
    "            # save best model (w.r.t validation loss)\n",
    "            if val_loss < best_model_score:\n",
    "                best_model = model.state_dict()\n",
    "                best_model_score = val_loss\n",
    "                best_model_epoch = e\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}, \\\n",
    "              Learning Rate : {optimizer.param_groups[0]['lr'] :.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history, marker=best_model_epoch)\n",
    "\n",
    "    return history, best_model, best_model_score\n",
    "\n",
    "def concat_dataloaders(dataloader1, dataloader2):\n",
    "    concat_x = []\n",
    "    concat_y = []\n",
    "\n",
    "    for elem in dataloader1:\n",
    "        for i in range(len(elem[0])):\n",
    "            concat_x.append(elem[0][i].tolist())\n",
    "            concat_y.append(elem[1][i].tolist())\n",
    "    for elem in dataloader2:\n",
    "        for i in range(len(elem[0])):\n",
    "            concat_x.append(elem[0][i].tolist())\n",
    "            concat_y.append(elem[1][i].tolist())\n",
    "    \n",
    "    x_tensor = torch.tensor(concat_x, device=device)\n",
    "    y_tensor = torch.tensor(concat_y, device=device)\n",
    "    dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "    iter = torch.utils.data.DataLoader(dataset = dataset, batch_size = 512, shuffle = True)\n",
    "\n",
    "    return iter  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sentiment140 (C:/Users/atace/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0/f81c014152931b776735658d8ae493b181927de002e706c4d5244ecb26376997)\n",
      "100%|██████████| 2/2 [00:00<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data after first step of preprocessing:  70000\n",
      "Tokenizing the data...\n",
      "Length of the data :  70000\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (C:/Users/atace/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 500.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data after first step of preprocessing:  70042\n",
      "Tokenizing the data...\n",
      "Length of the data :  70042\n",
      "1\n",
      "text         [[CLS], wicked, ##bit, ##ch, it, is, because, ...\n",
      "sentiment                                                    1\n",
      "len                                                         15\n",
      "Name: 1102814, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56000/56000 [00:02<00:00, 20385.48it/s]\n",
      "100%|██████████| 7000/7000 [00:00<00:00, 20404.34it/s]\n",
      "100%|██████████| 7000/7000 [00:00<00:00, 19562.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "text         [[CLS], elegant, ##ly, appointed, [SEP]]\n",
      "sentiment                                           1\n",
      "len                                                 5\n",
      "Name: 60611, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56033/56033 [00:03<00:00, 17370.17it/s]\n",
      "100%|██████████| 7004/7004 [00:00<00:00, 19105.34it/s]\n",
      "100%|██████████| 7005/7005 [00:00<00:00, 18830.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "Vocabulary Size :  20369\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "dataset = load_dataset(\"sentiment140\")\n",
    "data_train = pd.DataFrame({'text': dataset['train']['text'], 'sentiment' : dataset['train']['sentiment']})\n",
    "data_test = pd.DataFrame({'text': dataset['test']['text'], 'sentiment' : dataset['test']['sentiment']})\n",
    "\n",
    "concat = [data_train, data_test]\n",
    "data = pd.concat(concat, ignore_index=True)\n",
    "data = data.sample(n = 70000, random_state = 42)\n",
    "# convert string label to binary (int) label (positive:1, negative:0)\n",
    "data[\"sentiment\"] = data['sentiment'].apply(lambda x : int(x == 4))\n",
    "# train, test, val split\n",
    "train_A, valid_A, test_A = loader.make_dataset(data)\n",
    "\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "data_train = pd.DataFrame({'text': dataset['train']['sentence'], 'sentiment' : dataset['train']['label']})\n",
    "data_test = pd.DataFrame({'text': dataset['test']['sentence'], 'sentiment' : dataset['test']['label']})\n",
    "data_val = pd.DataFrame({'text': dataset['validation']['sentence'], 'sentiment' : dataset['validation']['label']})\n",
    "\n",
    "concat = [data_train, data_test, data_val]\n",
    "data = pd.concat(concat, ignore_index=True)\n",
    "data = data.sample(n = len(data), random_state = 42)\n",
    "# convert string label to binary (int) label (positive:1, negative:0)\n",
    "data[\"sentiment\"] = data['sentiment'].apply(lambda x : int(x == 1))\n",
    "# train, test, val split\n",
    "train_B, valid_B, test_B = loader.make_dataset(data)\n",
    "\n",
    "vocab = loader.get_vocab(pd.concat([train_A, train_B], ignore_index=True).iloc[:, 0])\n",
    "\n",
    "train_iter_A, valid_iter_A, test_iter_A = loader.make_iter(train_A, valid_A, test_A,\n",
    "                                                     batch_size=512,\n",
    "                                                     device=device,\n",
    "                                                     vocab=vocab)\n",
    "\n",
    "train_iter_B, valid_iter_B, test_iter_B = loader.make_iter(train_B, valid_B, test_B,\n",
    "                                                     batch_size=512,\n",
    "                                                     device=device,\n",
    "                                                     vocab=vocab)\n",
    "\n",
    "# NLP stuff\n",
    "pad_idx = vocab['__PAD__']\n",
    "voc_size = len(vocab)\n",
    "print(\"Vocabulary Size : \", voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mixed validation and test set\n",
    "train_iter = concat_dataloaders(train_iter_A, train_iter_B)\n",
    "valid_iter = concat_dataloaders(valid_iter_A, valid_iter_B)\n",
    "test_iter = concat_dataloaders(test_iter_A, test_iter_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding matrices we use the embedding B for the fused model since this embedding has learnt from the whole vocab.\n",
    "embeddingA = torch.load(\"Models/embeddingA_16_trained_downstream.pt\")\n",
    "embeddingB = torch.load(\"Models/embeddingB_16_trained_downstream.pt\")\n",
    "embedding = torch.load(\"Models/embeddingB_16_trained_downstream.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(\n",
       "        (embedding): Embedding(20369, 16)\n",
       "      )\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.7, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_k): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_v): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_concat): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.7, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.7, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.7, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding=embeddingA,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 2,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.7,\n",
    "                              device = device)\n",
    "\n",
    "modelA.load_state_dict(torch.load(\"./Models/modelA_sentiment140_256\"))\n",
    "modelA.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(\n",
       "        (embedding): Embedding(20369, 16)\n",
       "      )\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.7, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_k): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_v): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_concat): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.7, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.7, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.7, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding=embeddingB,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 2,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.7,\n",
    "                              device = device)\n",
    "\n",
    "modelB.load_state_dict(torch.load(\"./Models/modelB_sst2_256\"))\n",
    "modelB.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSupport(model, trainloader, l, alignment = \"acts\", numOfBatches= 10):\n",
    "    '''\n",
    "    Get the support matrices using Activation-based (\"acts\") or Weight-based (\"wts\") alignment \n",
    "    '''\n",
    "    if alignment == \"acts\":\n",
    "        activation = None\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if i >= numOfBatches:\n",
    "                break\n",
    "            \n",
    "            inputs, targets = data\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if activation is None:\n",
    "                activation = model.actMatrix[l]\n",
    "            else:\n",
    "                activation = torch.cat((activation, model.actMatrix[l]))\n",
    "\n",
    "        return activation\n",
    "    elif alignment == \"wts\":\n",
    "        return model.state_dict()[l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion(nameA, nameB, weightA, weightB, train_iter_sentiment140, train_iter_sst2, transport_matrix, beta, a):\n",
    "    support_y = getSupport(modelB, train_iter_sst2, nameB, alignment=\"wts\")\n",
    "    # Get the weights at layer \"idx\" from the first model\n",
    "    W_A = weightA\n",
    "    W_B = weightB\n",
    "    # Align the weights from the first model\n",
    "    aligned_W = torch.matmul(W_A, torch.matmul(transport_matrix, torch.diag(1 / beta)))\n",
    "    # Get the X-Support\n",
    "    n = W_A.shape[0]\n",
    "    alpha = torch.ones(n) * (1/n)\n",
    "    support_x = getSupport(modelA, train_iter_sentiment140, nameA, alignment=\"wts\")\n",
    "    # Calculate the euclidean distance between the supports\n",
    "    distance = ot.dist(support_x, support_y)\n",
    "    # Calculate beta\n",
    "    m = W_B.shape[0]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    # Calculate the transport matrix using optimal transport\n",
    "    transport_matrix = torch.from_numpy(ot.emd(alpha.numpy(), beta.numpy(), distance.detach().numpy())).float().reshape((n, m))\n",
    "    # Align model neurons\n",
    "    aligned_model = torch.matmul(torch.diag(1 / beta), torch.matmul(transport_matrix.T, aligned_W))\n",
    "    # Get the weights at layer \"idx\" from the second model\n",
    "    fused = (a * aligned_model + (1 - a) * W_B)\n",
    "    return  fused, transport_matrix, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_multihead(nameA, nameB, weightA, weightB, transport_matrix, beta, head):\n",
    "    support_y = weightB\n",
    "    support_x = weightA\n",
    "    # Get the weights at layer \"idx\" from the first model\n",
    "    W_A = weightA\n",
    "    W_B = weightB\n",
    "    # Initialize the fused model and transport matrix\n",
    "    fused = torch.empty(W_B.shape)\n",
    "    transport_matrix_new = torch.zeros((weightA.shape[0], weightB.shape[0]))\n",
    "    stride = weightB.shape[0] // head\n",
    "    for i in range(0, weightB.shape[0], stride):\n",
    "        # Align the weights from the first model\n",
    "        aligned_W = torch.matmul(W_A[i:i+stride, :], torch.matmul(transport_matrix, torch.diag(1 / beta)))\n",
    "        # Get the X-Support\n",
    "        n = W_A.shape[0] // head\n",
    "        alpha = torch.ones(n) * (1/n)\n",
    "        # Calculate the euclidean distance between the supports\n",
    "        distance = ot.dist(support_x[i:i+stride, :], support_y[i:i+stride, :])\n",
    "        # Calculate beta\n",
    "        m = W_B.shape[0] // head\n",
    "        beta_new = torch.ones(m) * (1/m)\n",
    "        # Calculate the transport matrix using optimal transport\n",
    "        transport_matrix_new[i:i+stride, i:i+stride] = torch.from_numpy(ot.emd(alpha.numpy(), beta_new.numpy(), distance.detach().numpy())).float().reshape((n, m))\n",
    "        # Align model neurons\n",
    "        aligned_model = torch.matmul(torch.diag(1 / beta_new), torch.matmul(transport_matrix_new[i:i+stride, i:i+stride].T, aligned_W))\n",
    "        # Get the weights at layer \"idx\" from the second model\n",
    "        fused[i:i+stride, :] = (aligned_model + W_B[i:i+stride, :]) / 2 \n",
    "    return  fused, transport_matrix_new, beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_crossmultihead(nameA, nameB, weightA, weightB, train_iter_sentiment140, train_iter_sst2, transport_matrix, beta, head):\n",
    "    W_A_head = weightA.view(head, -1)\n",
    "    W_B_head = weightB.view(head, -1)\n",
    "    \n",
    "    m = W_B_head.shape[1]\n",
    "    beta_head = torch.ones(m) * (1/m)\n",
    "    transport_matrix_head = torch.matmul(torch.diag(beta_head), torch.eye(m))\n",
    "\n",
    "    support_y = getSupport(modelB, train_iter_sst2, nameB, alignment=\"wts\")\n",
    "    support_x = getSupport(modelA, train_iter_sentiment140, nameA, alignment=\"wts\")\n",
    "\n",
    "    aligned_W = torch.matmul(W_A_head, torch.matmul(transport_matrix_head, torch.diag(1 / beta_head)))\n",
    "\n",
    "    dist_head = ot.dist(support_x.view(head, -1), support_y.view(head, -1))\n",
    "\n",
    "    n = W_A_head.shape[0]\n",
    "    alpha_head = torch.ones(n) * (1/n)\n",
    "\n",
    "    m = W_B_head.shape[0]\n",
    "    beta_head = torch.ones(m) * (1/m)\n",
    "\n",
    "    transport_matrix_new = torch.from_numpy(ot.emd(alpha_head.numpy(), beta_head.numpy(), dist_head.detach().numpy())).float().reshape((n, m))\n",
    "\n",
    "    aligned_W_A = torch.matmul(torch.diag(1 / beta_head), torch.matmul(transport_matrix_new.T, aligned_W))\n",
    "    aligned_W_A = aligned_W_A.view(weightA.shape)\n",
    "    return fusion_multihead(nameA, nameB, aligned_W_A, weightB, transport_matrix, beta, head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion via Optimal Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusedModel = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding = embedding,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 2,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.2,\n",
    "                              device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    a = trial.suggest_float('a', 0, 1)\n",
    "    \n",
    "    # Create the fused weights matrix\n",
    "    W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "    # Initialize the algorithm\n",
    "    m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "\n",
    "    # Fusion via Optimal Transport\n",
    "    for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "        if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "            W_fusion[nameA] = weightA\n",
    "        else:\n",
    "            if \"weight\" in nameA:\n",
    "                if \"encoder\" in nameA:\n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        W_fusion[nameA], transport_matrix_triplet, _ = fusion(nameA, nameB, weightA, weightB, train_iter_A, train_iter_B, transport_matrix, beta, a)\n",
    "                    else:\n",
    "                        W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, train_iter_A, train_iter_B, transport_matrix, beta, a)\n",
    "\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            elif \"bias\" in nameA:\n",
    "                if \"encoder\" in nameA: \n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                    else:\n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "\n",
    "    # Assign the weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in fusedModel.named_parameters():\n",
    "            param.data = torch.nn.Parameter(W_fusion[name])\n",
    "\n",
    "    # Validate the fused model\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_loss, val_acc, val_f1 = validation(fusedModel, valid_iter, criterion, device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-16 11:30:26,014]\u001b[0m A new study created in memory with name: no-name-1572e613-f4a6-4d9c-a1fc-45c20932abe2\u001b[0m\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "\u001b[32m[I 2023-01-16 11:30:55,031]\u001b[0m Trial 0 finished with value: 0.7823393302304404 and parameters: {'a': 0.5082627878576365}. Best is trial 0 with value: 0.7823393302304404.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:31:23,387]\u001b[0m Trial 1 finished with value: 0.918323448726109 and parameters: {'a': 0.12506235112925312}. Best is trial 0 with value: 0.7823393302304404.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:31:51,491]\u001b[0m Trial 2 finished with value: 0.9524152747222355 and parameters: {'a': 0.07653535702242442}. Best is trial 0 with value: 0.7823393302304404.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:32:24,251]\u001b[0m Trial 3 finished with value: 0.8996592547212329 and parameters: {'a': 0.9099749190409252}. Best is trial 0 with value: 0.7823393302304404.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:32:58,390]\u001b[0m Trial 4 finished with value: 0.7780478660549436 and parameters: {'a': 0.5262243443668826}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:33:33,248]\u001b[0m Trial 5 finished with value: 0.999623070870127 and parameters: {'a': 0.011208320184159737}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:34:11,795]\u001b[0m Trial 6 finished with value: 0.825600151504789 and parameters: {'a': 0.7148803538134307}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:34:45,599]\u001b[0m Trial 7 finished with value: 0.7937949746847153 and parameters: {'a': 0.643610944657544}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:35:22,351]\u001b[0m Trial 8 finished with value: 0.8921602751527514 and parameters: {'a': 0.9760971459207773}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:35:54,541]\u001b[0m Trial 9 finished with value: 0.8213442606585366 and parameters: {'a': 0.7189654048396694}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:36:24,606]\u001b[0m Trial 10 finished with value: 0.8358602864401681 and parameters: {'a': 0.30264797578818414}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:36:53,615]\u001b[0m Trial 11 finished with value: 0.7896707845585687 and parameters: {'a': 0.4500488865325179}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:37:24,120]\u001b[0m Trial 12 finished with value: 0.787566431931087 and parameters: {'a': 0.45365030988628263}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:37:53,766]\u001b[0m Trial 13 finished with value: 0.8514065742492676 and parameters: {'a': 0.25967260757379884}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:38:29,827]\u001b[0m Trial 14 finished with value: 0.7801994276898248 and parameters: {'a': 0.5736104661982881}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:39:07,962]\u001b[0m Trial 15 finished with value: 0.7816584152834756 and parameters: {'a': 0.5959025890184931}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:39:42,309]\u001b[0m Trial 16 finished with value: 0.8783882984093258 and parameters: {'a': 0.8282986781380998}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:40:17,277]\u001b[0m Trial 17 finished with value: 0.8267638214996883 and parameters: {'a': 0.31957872427143974}. Best is trial 4 with value: 0.7780478660549436.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:40:49,333]\u001b[0m Trial 18 finished with value: 0.7765631547995976 and parameters: {'a': 0.543161721444678}. Best is trial 18 with value: 0.7765631547995976.\u001b[0m\n",
      "\u001b[32m[I 2023-01-16 11:41:21,455]\u001b[0m Trial 19 finished with value: 0.8057942454304013 and parameters: {'a': 0.38854080103428135}. Best is trial 18 with value: 0.7765631547995976.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.7766070876802716\n",
      "Validation Accuracy:      0.6497163318452381\n",
      "Validation F1:      tensor(0.6493)\n"
     ]
    }
   ],
   "source": [
    "a = study.best_params[\"a\"]\n",
    "\n",
    "# Create the fused weights matrix\n",
    "W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "# Initialize the algorithm\n",
    "m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "beta = torch.ones(m) * (1/m)\n",
    "transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "# Fusion via Optimal Transport\n",
    "for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "    if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "        W_fusion[nameA] = weightA\n",
    "    else:\n",
    "        if \"weight\" in nameA:\n",
    "            if \"encoder\" in nameA:\n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    W_fusion[nameA], transport_matrix_triplet, _ = fusion(nameA, nameB, weightA, weightB, train_iter_A, train_iter_B, transport_matrix, beta, a)\n",
    "                else:\n",
    "                    W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, train_iter_A, train_iter_B, transport_matrix, beta, a)\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        elif \"bias\" in nameA:\n",
    "            if \"encoder\" in nameA: \n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        else:\n",
    "            W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "# Assign the weights\n",
    "with torch.no_grad():\n",
    "    for name, param in fusedModel.named_parameters():\n",
    "        param.data = torch.nn.Parameter(W_fusion[name])\n",
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc, val_f1 = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())\n",
    "print(\"Validation F1:     \", val_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibrate the Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/219 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/1843898165.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 219/219 [05:38<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(fusedModel.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "clip = 1\n",
    "\n",
    "fusedModel.train()\n",
    "for name, param in fusedModel.named_parameters():\n",
    "    if \"weight\" in name or \"bias\" in name:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "for i, batch in enumerate(tqdm(train_iter)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = fusedModel(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(fusedModel.parameters(), clip)\n",
    "            optimizer.step() # optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.6135832348040172\n",
      "Validation Accuracy:      0.6727523561507937\n",
      "Validation F1:      tensor(0.6740)\n"
     ]
    }
   ],
   "source": [
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc, val_f1 = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())\n",
    "print(\"Validation F1:     \", val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in fusedModel.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/219 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2985598703.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 219/219 [05:38<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(fusedModel.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "clip = 1\n",
    "\n",
    "fusedModel.train()\n",
    "for name, param in fusedModel.named_parameters():\n",
    "    if \"encoder\" in name:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "for i, batch in enumerate(tqdm(train_iter)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = fusedModel(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(fusedModel.parameters(), clip)\n",
    "            optimizer.step() # optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.6207659414836338\n",
      "Validation Accuracy:      0.6584418402777777\n",
      "Validation F1:      tensor(0.6592)\n"
     ]
    }
   ],
   "source": [
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc, val_f1 = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())\n",
    "print(\"Validation F1:     \", val_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss Fused Model:      0.620941834790366\n",
      "Test Accuracy Fused Model:      0.6547924785418312\n",
      "Test F1 Fused Model:      tensor(0.6543)\n",
      "Test Loss Model A:      0.653900778719357\n",
      "Test Accuracy Model A:      0.6947001251726519\n",
      "Test F1 Model A:      tensor(0.6938)\n",
      "Test Loss Model B:      0.7619793521506446\n",
      "Test Accuracy Model B:      0.767161139749408\n",
      "Test F1 Model B:      tensor(0.7670)\n"
     ]
    }
   ],
   "source": [
    "# Test the models on the mixed test set\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_fused, test_acc_fused, test_f1_fused = validation(fusedModel, test_iter, criterion, device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_A, test_acc_A, test_f1_A = validation(modelA, test_iter, criterion, device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss_B, test_acc_B, test_f1_B = validation(modelB, test_iter, criterion, device)\n",
    "\n",
    "print(\"Test Loss Fused Model:     \", test_loss_fused)\n",
    "print(\"Test Accuracy Fused Model:     \", test_acc_fused.item())\n",
    "print(\"Test F1 Fused Model:     \", test_f1_fused)\n",
    "print(\"Test Loss Model A:     \", test_loss_A)\n",
    "print(\"Test Accuracy Model A:     \", test_acc_A.item())\n",
    "print(\"Test F1 Model A:     \", test_f1_A)\n",
    "print(\"Test Loss Model B:     \", test_loss_B)\n",
    "print(\"Test Accuracy Model B:     \", test_acc_B.item())\n",
    "print(\"Test F1 Model B:     \", test_f1_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss Fused Model:      0.5854056434971946\n",
      "Test Accuracy Fused Model:      0.6935313278654486\n",
      "Test F1 Fused Model:      tensor(0.6936)\n",
      "Test Loss Model A:      0.5179040751286915\n",
      "Test Accuracy Model A:      0.7646289711378736\n",
      "Test F1 Model A:      tensor(0.7641)\n",
      "Test Loss Model B:      1.163452752998897\n",
      "Test Accuracy Model B:      0.6589266247923588\n",
      "Test F1 Model B:      tensor(0.6579)\n"
     ]
    }
   ],
   "source": [
    "# Test the models on the sentiment140 test set\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_fused, test_acc_fused, test_f1_fused = validation(fusedModel, test_iter_A, criterion, device)\n",
    "test_loss_A, test_acc_A, test_f1_A = validation(modelA, test_iter_A, criterion, device)\n",
    "test_loss_B, test_acc_B, test_f1_B = validation(modelB, test_iter_A, criterion, device)\n",
    "\n",
    "print(\"Test Loss Fused Model:     \", test_loss_fused)\n",
    "print(\"Test Accuracy Fused Model:     \", test_acc_fused.item())\n",
    "print(\"Test F1 Fused Model:     \", test_f1_fused)\n",
    "print(\"Test Loss Model A:     \", test_loss_A)\n",
    "print(\"Test Accuracy Model A:     \", test_acc_A.item())\n",
    "print(\"Test F1 Model A:     \", test_f1_A)\n",
    "print(\"Test Loss Model B:     \", test_loss_B)\n",
    "print(\"Test Accuracy Model B:     \", test_acc_B.item())\n",
    "print(\"Test F1 Model B:     \", test_f1_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_21784/2549446300.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss Fused Model:      0.6563881507941655\n",
      "Test Accuracy Fused Model:      0.6153483006293491\n",
      "Test F1 Fused Model:      tensor(0.6151)\n",
      "Test Loss Model A:      0.7951585182121822\n",
      "Test Accuracy Model A:      0.6231140311604585\n",
      "Test F1 Model A:      tensor(0.6234)\n",
      "Test Loss Model B:      0.3634702478136335\n",
      "Test Accuracy Model B:      0.8761044789961113\n",
      "Test F1 Model B:      tensor(0.8761)\n"
     ]
    }
   ],
   "source": [
    "# Test the models on the Stanford Treebank test set\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_fused, test_acc_fused, test_f1_fused = validation(fusedModel, test_iter_B, criterion, device)\n",
    "test_loss_A, test_acc_A, test_f1_A = validation(modelA, test_iter_B, criterion, device)\n",
    "test_loss_B, test_acc_B, test_f1_B = validation(modelB, test_iter_B, criterion, device)\n",
    "\n",
    "print(\"Test Loss Fused Model:     \", test_loss_fused)\n",
    "print(\"Test Accuracy Fused Model:     \", test_acc_fused.item())\n",
    "print(\"Test F1 Fused Model:     \", test_f1_fused)\n",
    "print(\"Test Loss Model A:     \", test_loss_A)\n",
    "print(\"Test Accuracy Model A:     \", test_acc_A.item())\n",
    "print(\"Test F1 Model A:     \", test_f1_A)\n",
    "print(\"Test Loss Model B:     \", test_loss_B)\n",
    "print(\"Test Accuracy Model B:     \", test_acc_B.item())\n",
    "print(\"Test F1 Model B:     \", test_f1_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
