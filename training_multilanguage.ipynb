{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from dataloader import *\n",
    "from transformer import *\n",
    "from bleu import *\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "import string\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "dataloader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "# load English data\n",
    "filenames = 'Data/multi30k-dataset/data/task1/tok/train.lc.norm.tok.en'\n",
    "textENG = dataloader.load_doc(filenames)\n",
    "sentencesENG = dataloader.to_sentence(textENG)\n",
    "\n",
    "# load French data\n",
    "filenames = 'Data/multi30k-dataset/data/task1/tok/train.lc.norm.tok.fr'\n",
    "textFR = dataloader.load_doc(filenames)\n",
    "sentencesFR = dataloader.to_sentence(textFR)\n",
    "\n",
    "# load German data\n",
    "filenames ='Data/multi30k-dataset/data/task1/tok/train.lc.norm.tok.de'\n",
    "textDEU = dataloader.load_doc(filenames)\n",
    "sentencesDEU = dataloader.to_sentence(textDEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_eng(text):\n",
    "   return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "    \n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in tqdm(lines):\n",
    "        vocab.update(line)\n",
    "    return vocab\n",
    "\n",
    "def trim_vocab(vocab, min_occurance):\n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "    tokens = ['__PAD__', '__SOS__', '__UNK__'] + tokens \n",
    "    return {tok : idx for idx, tok in enumerate(tokens)}\n",
    "\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        new_tokens.append(vocab['__SOS__'])\n",
    "        for token in line:\n",
    "            if token in vocab.keys():\n",
    "                new_tokens.append(vocab[token])\n",
    "            else:\n",
    "                new_tokens.append(vocab['__UNK__'])\n",
    "        while len(new_tokens) < 64:\n",
    "            new_tokens.append(vocab['__PAD__'])\n",
    "        new_lines.append(new_tokens)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29000/29000 [00:00<00:00, 169601.52it/s]\n",
      "100%|██████████| 29000/29000 [00:00<00:00, 184704.32it/s]\n",
      "100%|██████████| 29000/29000 [00:00<00:00, 202325.44it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_eng = to_vocab([tokenize_eng(sentence) for sentence in sentencesENG])\n",
    "vocab_eng = trim_vocab(vocab_eng, 2)\n",
    "tokenized_eng = update_dataset([tokenize_eng(sentence) for sentence in sentencesENG], vocab_eng)\n",
    "\n",
    "vocab_ger = to_vocab([tokenize_ger(sentence) for sentence in sentencesDEU])\n",
    "vocab_ger = trim_vocab(vocab_ger, 2)\n",
    "tokenized_ger = update_dataset([tokenize_ger(sentence) for sentence in sentencesDEU], vocab_ger)\n",
    "\n",
    "vocab_fr = to_vocab([tokenize_fr(sentence) for sentence in sentencesFR])\n",
    "vocab_fr = trim_vocab(vocab_fr, 2)\n",
    "tokenized_fr = update_dataset([tokenize_fr(sentence) for sentence in sentencesFR], vocab_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Iterator Generator\n",
    "def create_Iterators(source, target, batch_size, train_size=0.9):\n",
    "    df = pd.DataFrame(data = {\"source\": source, \"target\": target})\n",
    "\n",
    "    train, rem = train_test_split(df, train_size=train_size, random_state = 42)\n",
    "    valid_size = 0.5\n",
    "    valid, test = train_test_split(rem, train_size=valid_size, random_state = 42)\n",
    "    \n",
    "    train_x, train_y = torch.tensor(list(train[\"source\"].values)), torch.tensor(list(train[\"target\"].values))\n",
    "    valid_x, valid_y = torch.tensor(list(valid[\"source\"].values)), torch.tensor(list(valid[\"target\"].values))\n",
    "    test_x, test_y = torch.tensor(list(test[\"source\"].values)), torch.tensor(list(test[\"target\"].values))\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    validate = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
    "    test = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "\n",
    "    train_iterator = torch.utils.data.DataLoader(dataset = train, batch_size = batch_size, shuffle = True)\n",
    "    valid_iterator = torch.utils.data.DataLoader(dataset = validate, batch_size = batch_size, shuffle = True)\n",
    "    test_iterator = torch.utils.data.DataLoader(dataset = test, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    return train_iterator, valid_iterator, test_iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_A, valid_iter_A, test_iter_A = create_Iterators(tokenized_fr, tokenized_eng, batch_size=64)\n",
    "train_iter_B, valid_iter_B, test_iter_B = create_Iterators(tokenized_ger, tokenized_eng, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   1,    3,  104,  ...,    0,    0,    0],\n",
       "         [   1,   18,   29,  ...,    0,    0,    0],\n",
       "         [   1,   70,  144,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   1,   18,   29,  ...,    0,    0,    0],\n",
       "         [   1,   87, 2531,  ...,    0,    0,    0],\n",
       "         [   1,   18,   94,  ...,    0,    0,    0]]),\n",
       " tensor([[   1,    3, 2681,  ...,    0,    0,    0],\n",
       "         [   1,   20,   30,  ...,    0,    0,    0],\n",
       "         [   1,  117,    8,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   1,   30,   63,  ...,    0,    0,    0],\n",
       "         [   1,   83, 1527,  ...,    0,    0,    0],\n",
       "         [   1,   20,   89,  ...,    0,    0,    0]])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_enc_A = torch.nn.Embedding(len(vocab_fr), 128)\n",
    "embedding_dec_A = torch.nn.Embedding(len(vocab_eng), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformerA = Transformer(\n",
    "    src_pad_idx = vocab_fr['__PAD__'],\n",
    "    trg_pad_idx = vocab_eng['__PAD__'],\n",
    "    trg_sos_idx = vocab_eng['__SOS__'],\n",
    "    embedding_enc = embedding_enc_A,\n",
    "    embedding_dec = embedding_dec_A,\n",
    "    enc_voc_size = len(vocab_fr), \n",
    "    dec_voc_size = len(vocab_eng), \n",
    "    d_model = 128, \n",
    "    n_head = 8, \n",
    "    max_len = 72,\n",
    "    ffn_hidden = 512, \n",
    "    n_layers = 2, \n",
    "    drop_prob = 0.2, \n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "        print(src.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        #print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, target_vocab, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            total_bleu = []\n",
    "            for j in range(len(batch[0])):\n",
    "                trg_words = idx_to_word(batch[1][j], target_vocab)\n",
    "                output_words = output[j].max(dim=1)[1]\n",
    "                output_words = idx_to_word(output_words, target_vocab)\n",
    "                bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n",
    "                total_bleu.append(bleu)\n",
    "\n",
    "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            batch_bleu.append(total_bleu)\n",
    "\n",
    "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len(iterator), batch_bleu\n",
    "\n",
    "\n",
    "def run(model, train_iter, valid_iter, target_vocab, optimizer, criterion, clip, scheduler, total_epoch, best_loss):\n",
    "    train_losses, test_losses, bleus = [], [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "        valid_loss, bleu = evaluate(model, valid_iter, target_vocab, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if step > 5:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        bleus.append(bleu)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # if valid_loss < best_loss:\n",
    "        #     best_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
    "\n",
    "        # f = open('result/train_loss.txt', 'w')\n",
    "        # f.write(str(train_losses))\n",
    "        # f.close()\n",
    "\n",
    "        # f = open('result/bleu.txt', 'w')\n",
    "        # f.write(str(bleus))\n",
    "        # f.close()\n",
    "\n",
    "        # f = open('result/test_loss.txt', 'w')\n",
    "        # f.write(str(test_losses))\n",
    "        # f.close()\n",
    "\n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print(f'\\tBLEU Score: {bleu:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/408 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/408 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7652/3705451430.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                                  patience=5)\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m run(transformerA, \n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_iter_A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mvalid_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_iter_A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7652/64488816.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(model, train_iter, valid_iter, target_vocab, optimizer, criterion, clip, scheduler, total_epoch, best_loss)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7652/64488816.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\atace\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\atace\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=transformerA.parameters(),\n",
    "                 lr=0.1,\n",
    "                 weight_decay=5e-4,\n",
    "                 eps=5e-9)\n",
    "criterion = criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr['__PAD__'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=0.5,\n",
    "                                                 patience=5)\n",
    "\n",
    "run(transformerA, \n",
    "    train_iter=train_iter_A,\n",
    "    valid_iter=valid_iter_A, \n",
    "    target_vocab=vocab_eng, \n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion, \n",
    "    clip=1.0, \n",
    "    scheduler=scheduler,\n",
    "    total_epoch=20,\n",
    "    best_loss=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
