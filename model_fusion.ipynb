{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import spacy\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, src_pad_idx, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.encoder = Encoder(enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device)\n",
    "        self.linear = nn.Linear(d_model * max_len, 2).to(device)\n",
    "\n",
    "    def make_pad_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.make_pad_mask(x, x)\n",
    "        x = self.encoder(x, mask)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training(history, marker=None):\n",
    "  plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.01,\n",
    "                    right=1.5,\n",
    "                    top=0.6,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(history['train_loss'])\n",
    "  plt.plot(history['val_loss'])\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training loss')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_loss'] + history['val_loss'])\n",
    "    y_max = max(history['train_loss'] + history['val_loss'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(history['train_acc'])\n",
    "  plt.plot(history['val_acc'])\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training metric')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_acc'] + history['val_acc'])\n",
    "    y_max = max(history['train_acc'] + history['val_acc'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, iterator, optimizer, criterion, device):\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # validation\n",
    "    # loss, metrics for current epoch\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # stop graph\n",
    "        # batches\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            output = model(src)\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_accuracy += accuracy\n",
    "\n",
    "    # return mean loss w.r.t. batches\n",
    "    return val_epoch_loss / len(iterator), val_epoch_accuracy / len(iterator)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': []}\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "          for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc]):\n",
    "            history[key].append(value)\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history)\n",
    "\n",
    "    return history\n",
    "\n",
    "def train_save_best(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': [],\n",
    "               'learning_rate': []}\n",
    "    best_model = None\n",
    "    best_model_score = 1e9\n",
    "    best_model_epoch = 0\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc, current_lr]):\n",
    "                history[key].append(value)\n",
    "\n",
    "            # save best model (w.r.t validation loss)\n",
    "            if val_loss < best_model_score:\n",
    "                best_model = model.state_dict()\n",
    "                best_model_score = val_loss\n",
    "                best_model_epoch = e\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history, marker=best_model_epoch)\n",
    "\n",
    "    return history, best_model, best_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# init\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "# import data (combine train/test as we split afterwards anyways)\n",
    "data = pd.concat([pd.read_csv(\"./Data/SMS_train.csv\", encoding='ISO-8859-1'),\n",
    "                  pd.read_csv(\"./Data/SMS_test.csv\", encoding='ISO-8859-1')])\n",
    "\n",
    "# convert string label to binary (int) label (spam:1, non-spam:0)\n",
    "labels = pd.Series((data['Label'] == 'Spam').astype(int))\n",
    "data['Label'] = labels\n",
    "\n",
    "# train, test, val split\n",
    "train, valid, test = loader.make_dataset(data[['Message_body', 'Label']])\n",
    "vocab = loader.get_vocab(train.iloc[:, 0])\n",
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=128,\n",
    "                                                     device=device)\n",
    "\n",
    "# NLP stuff\n",
    "pad_idx = vocab['__PAD__']\n",
    "voc_size = len(vocab)\n",
    "print(\"Vocabulary Size : \", voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Idea: We train model A and model B for long enough, s.t. they start overfitting. We use their best models w.r.t. validation set (i.e. not the final model after all training epochs) and fuse them together. The fused model is then trained for long enough as well, saving the best model w.r.t to the same validation set. The fused model is then compared with its parent models on the separate test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "note that dataset is imbalanced -> accuracy is not a good metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model A\n",
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "optA = torch.optim.Adam(modelA.parameters(), lr=0.001)\n",
    "loss_fnA = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 700\n",
    "\n",
    "historyA, best_modelA, best_model_scoreA = train_save_best(model=modelA,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optA,\n",
    "                                                            criterion=loss_fnA,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# save model\n",
    "torch.save(best_modelA, './Models/modelA')\n",
    "\n",
    "# save history\n",
    "with open('./Models/historyA.txt', 'w') as dat:\n",
    "    dat.write(str(historyA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model B\n",
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "optB = torch.optim.SGD(modelB.parameters(), lr=0.001)\n",
    "loss_fnB = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "historyB, best_modelB, best_model_scoreB = train_save_best(model=modelB,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optB,\n",
    "                                                            criterion=loss_fnB,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# save model\n",
    "torch.save(best_modelB, './Models/modelB')\n",
    "\n",
    "# save history\n",
    "with open('./Models/historyB.txt', 'w') as dat:\n",
    "    dat.write(str(historyB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weight_averaging(*weights):\n",
    "  with torch.no_grad():\n",
    "    sum = torch.zeros(weights[0].shape, device=device)\n",
    "    for weight in weights:\n",
    "      sum += weight\n",
    "  return sum / len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_averaging(*linears):\n",
    "  \"\"\"Averages several linear layers (weights + biases)\"\"\"\n",
    "  with torch.no_grad():\n",
    "    weights = [linear.weight for linear in linears]\n",
    "    biases = [linear.bias for linear in linears]\n",
    "\n",
    "    linear_averaged = torch.nn.Linear(linears[0].in_features, linears[0].out_features, bias=True).to(device)\n",
    "    linear_averaged.weight, linear_averaged.bias = torch.nn.Parameter(weight_averaging(*weights)), torch.nn.Parameter(weight_averaging(*biases))\n",
    "\n",
    "  return linear_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vanilla_fusion(modelA, modelB):\n",
    "  # init\n",
    "  model_fusion = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # 1) encoder\n",
    "    # TODO: smarter method for embedding\n",
    "    # a) embedding\n",
    "    weights_A = modelA.encoder.emb.tok_emb.weight\n",
    "    weights_B = modelB.encoder.emb.tok_emb.weight\n",
    "\n",
    "    weights_fusion = weight_averaging(weights_A, weights_B) # weights seem to be exactly the same?\n",
    "    model_fusion.encoder.emb.tok_emb.weight = torch.nn.Parameter(weights_fusion)\n",
    "\n",
    "    # b) encoder layers\n",
    "    for i, _ in enumerate(modelA.encoder.layers):\n",
    "      # i) self-attention (fuse Q, K, V separately) # TODO: check validity of this approach\n",
    "      # query\n",
    "      query_A = modelA.encoder.layers[i].attention.w_q\n",
    "      query_B = modelB.encoder.layers[i].attention.w_q\n",
    "\n",
    "      query_fusion = linear_averaging(query_A, query_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_q = query_fusion\n",
    "\n",
    "      # key\n",
    "      key_A = modelA.encoder.layers[i].attention.w_k\n",
    "      key_B = modelB.encoder.layers[i].attention.w_k\n",
    "\n",
    "      key_fusion = linear_averaging(key_A, key_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_k = key_fusion\n",
    "\n",
    "      # value\n",
    "      value_A = modelA.encoder.layers[i].attention.w_v\n",
    "      value_B = modelB.encoder.layers[i].attention.w_v\n",
    "\n",
    "      value_fusion = linear_averaging(value_A, value_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_v = value_fusion\n",
    "\n",
    "      # output\n",
    "      output_A = modelA.encoder.layers[i].attention.w_concat\n",
    "      output_B = modelB.encoder.layers[i].attention.w_concat\n",
    "\n",
    "      output_fusion = linear_averaging(output_A, output_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_concat = output_fusion\n",
    "\n",
    "      # ii) layer norm 1\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "      # iii) feed-forward network\n",
    "      # layer 1\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear1\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear1\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear1 = linear_fusion\n",
    "\n",
    "      # layer 2\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear2\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear2\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear2 = linear_fusion\n",
    "\n",
    "      # iv) layer norm 2\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "    # 2) MLP head\n",
    "    linear_A = modelA.linear\n",
    "    linear_B = modelB.linear\n",
    "\n",
    "    linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "    model_fusion.linear = linear_fusion\n",
    "\n",
    "  return model_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_fusion(modelA, modelB, model_fusion):\n",
    "  # test fusion\n",
    "  test_loss_A, test_acc_A = validation(modelA, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_B, test_acc_B = validation(modelB, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_fusion, test_acc_fusion = validation(model_fusion, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "  # visualize\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  metrics_A = [test_loss_A, test_acc_A]\n",
    "  metrics_B = [test_loss_B, test_acc_B]\n",
    "  metrics_fusion = [test_loss_fusion, test_acc_fusion]\n",
    "  metrics = ['loss', 'accuracy']\n",
    "  x = np.arange(len(metrics)) # positions of bars (1 per metric)\n",
    "  width = 0.25  # the width of the bars\n",
    "\n",
    "  rects1 = ax.bar(x - width, metrics_A, width, label='model A')\n",
    "  rects2 = ax.bar(x, metrics_B, width, label='model B')\n",
    "  rects3 = ax.bar(x + width, metrics_fusion, width, label='model fusion')\n",
    "\n",
    "  ax.set_ylabel('Score')\n",
    "  ax.set_title('Test metrics by models')\n",
    "  ax.set_xticks(x)\n",
    "  ax.set_xticklabels(metrics)\n",
    "  ax.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (Optional) load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load parent models\n",
    "# TODO: change enc_voc_size back\n",
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "modelA.load_state_dict(torch.load('./Models/modelA'))\n",
    "modelB.load_state_dict(torch.load('./Models/modelB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion = vanilla_fusion(modelA, modelB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with new randomly initialized transformer\n",
    "test_fusion(modelA, modelB, TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "opt_fusion = torch.optim.SGD(model_fusion.parameters(), lr=0.001)\n",
    "loss_fn_fusion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "history_fusion, best_model_fusion, best_model_score_fusion = train_save_best(model=model_fusion,\n",
    "                                                                             iterator=train_iter,\n",
    "                                                                             optimizer=opt_fusion,\n",
    "                                                                             criterion=loss_fn_fusion,\n",
    "                                                                             epoch=epochs,\n",
    "                                                                             clip=1,\n",
    "                                                                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion.load_state_dict(best_model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion (after retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
