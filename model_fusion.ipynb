{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import spacy\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training(history, marker=None):\n",
    "  plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.01,\n",
    "                    right=1.5,\n",
    "                    top=0.6,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(history['train_loss'])\n",
    "  plt.plot(history['val_loss'])\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training loss')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_loss'] + history['val_loss'])\n",
    "    y_max = max(history['train_loss'] + history['val_loss'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(history['train_acc'])\n",
    "  plt.plot(history['val_acc'])\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training metric')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_acc'] + history['val_acc'])\n",
    "    y_max = max(history['train_acc'] + history['val_acc'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, iterator, optimizer, criterion, device):\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # validation\n",
    "    # loss, metrics for current epoch\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # stop graph\n",
    "        # batches\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            output = model(src)\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_accuracy += accuracy\n",
    "\n",
    "    # return mean loss w.r.t. batches\n",
    "    return val_epoch_loss / len(iterator), val_epoch_accuracy / len(iterator)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': []}\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "          for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc]):\n",
    "            history[key].append(value)\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history)\n",
    "\n",
    "    return history\n",
    "\n",
    "def train_save_best(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': [],\n",
    "               'learning_rate': []}\n",
    "    best_model = None\n",
    "    best_model_score = 1e9\n",
    "    best_model_epoch = 0\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc, current_lr]):\n",
    "                history[key].append(value)\n",
    "\n",
    "            # save best model (w.r.t validation loss)\n",
    "            if val_loss < best_model_score:\n",
    "                best_model = model.state_dict()\n",
    "                best_model_score = val_loss\n",
    "                best_model_epoch = e\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history, marker=best_model_epoch)\n",
    "\n",
    "    return history, best_model, best_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "Tokenizing the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atace\\OneDrive\\Desktop\\ETH\\9.Semester\\Deep Learning\\project\\Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers\\dataloader.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"len\"] = data.iloc[:, 0].apply(lambda x : len(self.tokenize(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data :  6079\n",
      "1\n",
      "review       [[CLS], this, film, was, excellent, i, thought...\n",
      "sentiment                                                    1\n",
      "len                                                         78\n",
      "Name: 7163, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4863/4863 [00:00<00:00, 14136.69it/s]\n",
      "100%|██████████| 608/608 [00:00<00:00, 14818.28it/s]\n",
      "100%|██████████| 608/608 [00:00<00:00, 10135.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "Vocabulary Size :  14358\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "# import data (combine train/test as we split afterwards anyways)\n",
    "# data = pd.concat([pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1'),\n",
    "#                   pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1')])\n",
    "data = pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1')\n",
    "# convert string label to binary (int) label (spam:1, non-spam:0)\n",
    "data[\"sentiment\"] = data['sentiment'].apply(lambda x : int(x == \"positive\"))\n",
    "\n",
    "# train, test, val split\n",
    "train, valid, test = loader.make_dataset(data)\n",
    "vocab = loader.get_vocab(train.iloc[:, 0])\n",
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=128,\n",
    "                                                     device=device)\n",
    "\n",
    "# NLP stuff\n",
    "pad_idx = vocab['__PAD__']\n",
    "voc_size = len(vocab)\n",
    "print(\"Vocabulary Size : \", voc_size)\n",
    "\n",
    "# Creating the embedding matrix\n",
    "embedding = torch.nn.Embedding(voc_size, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Idea: We train model A and model B for long enough, s.t. they start overfitting. We use their best models w.r.t. validation set (i.e. not the final model after all training epochs) and fuse them together. The fused model is then trained for long enough as well, saving the best model w.r.t to the same validation set. The fused model is then compared with its parent models on the separate test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "note that dataset is imbalanced -> accuracy is not a good metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_13644/2041275992.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 38/38 [00:58<00:00,  1.54s/it]\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_13644/2041275992.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Train Loss: 1.5485               Validation Loss: 1.0094               Train acc: 0.5030,               Val acc: 0.5484\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApAAAADnCAYAAACpDWYAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAh9klEQVR4nO3de7gddX3v8feHsCUgUC4JggmSaHm04EHUTYSjPY1t9QFEwSPHRoHai1JsbcVqldZzwN6eaq9ewKZY8VIR5Igo5QRUrIBWUAIGBcSCVB42iAmRWypRAt/zx5rExWbty8pea699eb+eZz17Zn6/mfnOfna++a6Z38ykqpAkSZIma4dBByBJkqTZxQJSkiRJXbGAlCRJUlcsICVJktQVC0hJkiR1xQJSkiRJXbGAVF8luTTJ63rdt8sYViYZ6fV2JWnQZkKOHYQkJyT5wqDjmM/icyA1WpJNbbO7AD8BHm3mf6eqzp3+qLZfkpXAJ6pq6YBDkaQ5l2N7Kcky4D+BoaraMuBwNI4dBx2AZp6q2nXrdJLvA6+vqstH90uyo//AJak75tip8fcyM3gJW5O29VJwknckuQf4SJI9k1ySZEOS+5rppW3rXJHk9c30byT5apK/bfr+Z5KjtrPv8iRXJXkoyeVJzkryiUkexy80+7o/yU1JXtHWdnSSm5vt3pXkbc3yRc2x3Z/kR0m+ksR/P5J6Zrbm2La4355kfZIfJDmuyaf/0eTMP2nrv0OS05J8L8nGJBck2atpvqr5eX+STUmOaGL99yT/kORHwLu2xt+2zYOTfLHZ1w/b96f+8D9AdWtfYC/gAOBkWn9DH2nmnwY8DJw5zvovAL4LLAL+GvhwkmxH308C3wD2Bt4FnDSZ4JMMAf8KfAHYB/h94Nwkz2y6fJjWJaTdgGcD/9YsfyswAiwGngL8CeD4D0m9Nltz7L7AQmAJcDrwIeBE4PnALwKnJ3l60/cPgOOAXwKeCtwHnNW0/Y/m5x5VtWtVXd0W6+208vZftu84yW7A5cBlzfZ+HvjSBPFqiiwg1a3HgDOq6idV9XBVbayqC6vqx1X1EK1/2L80zvp3VNWHqupR4GPAfrQKskn3TfI04DDg9Kr6aVV9Fbh4kvEfDuwKvLtZ99+AS4DXNO2PAAcl2b2q7quq69uW7wccUFWPVNVXygHEknpvtubYR4C/rKpHgPNpFaXvq6qHquom4CbgkKbv7wDvrKqRqvoJrQL1+CTjDau7u6o+UFVbqurhUW3HAPdU1d9V1eZmn1+fIF5NkQWkurWhqjZvnUmyS5J/SnJHkgdpXX7YI8mCMda/Z+tEVf24mdy1y75PBX7UtgzgzknG/1Tgzqp6rG3ZHbS+NQO8CjgauCPJlUmOaJb/DXAb8IUktyc5bZL7k6RuzNYcu7EpRKF1lhTgh23tD7fFcQBwUTMk6H7gO7RuIhqr0J1o//sD35sgPvWYBaS6Nfqs21uBZwIvqKrd+dnlh7EumfTCD4C9kuzStmz/Sa57N7D/qPGLTwPuAqiqa6vqWFqXST4LXNAsf6iq3lpVTwdeDvxhkl+Z2mFI0hPM9hw7GXcCR1XVHm2fhVV1F2MPDRrvis+dwDN6GJ8mwQJSU7UbrW+W9zeDoM/o9w6r6g5gLa2B1E9qzhK+fJKrfx34L+DtSYbSesTPy4Hzm22dkOTnmsswD9I8WiPJMUl+vhkftHX5ox33IEm9M9ty7GSsBv4yyQEASRYnObZp20DrMv7Tx1q5g0uAfZOcmmSnJLsleUEP41UHFpCaqvcCOwP3AtfQGsQ8HU4AjgA2An8BfIrWs9TGVVU/BV4BHEUr5g8Cv15VtzRdTgK+31wqOoXWIHCAA2kN0t4EXA18sKqu6NXBSNIY3sssyrGT9D5aYyq/kOQhWsf1Ath2Kf0vgX9vLnEfPtHGmrGhL6FV5N4D3Aq8uEexagw+SFxzQpJPAbdUVd+/nUvSfGOO1WiegdSslOSwJM9onid2JHAsrTGLkqQpMsdqIr6JRrPVvsBnaD2jbAR4Y1V9c7AhSdKcYY7VuLyELUmSpK54CVuSJEldsYCUpFksyZFJvpvktk4PuG/eU/xAknXN5/S2tu8n+XazfO30Ri5pNpt1YyAXLVpUy5YtG3QYkqbBddddd29VLR50HDNV8zaSs2g9wmQEuDbJxVV186iuX6mqY8bYzIur6t7J7tMcLM0f4+XgWVdALlu2jLVr/aIszQdJ7hh0DDPcCuC2qrodIMn5tO6WHV1A9ow5WJo/xsvBXsKWpNlrCY9/R/AIP3uve7sjktyQ5NIkB7ctL1oPc74uyclj7STJyUnWJlm7YcOG3kQuaVabdWcgJUnbdHof8uhHa1wPHFBVm5IcTetZfgc2bS+sqruT7AN8McktVXXVEzZYdTZwNsDw8LCP7pDkGUhJmsVGgP3b5pcCd7d3qKoHq2pTM70GGEqyqJm/u/m5HriI1iVxSZrQnDgD+cgjjzAyMsLmzZsHHUrfLVy4kKVLlzI0NDToUCQN3rXAgUmWA3cBq4DXtndIsi/ww6qqJCtonTjYmOTJwA5V9VAz/VLgz7YniPmSg82/0s/MiQJyZGSE3XbbjWXLlpF0uqIzN1QVGzduZGRkhOXLlw86HEkDVlVbkrwJ+DywADinqm5KckrTvho4Hnhjki3Aw8Cqpph8CnBRkzN3BD5ZVZdtTxzzIQebf6XHmxMF5ObNm+d04toqCXvvvTcOYpe0VXNZes2oZavbps8Ezuyw3u3Ac3oRw3zIweZf6fHmzBjIuZy42s2X43yclStbH0kz1nzITfPhGDsyB6uDOVNADtL999/PBz/4wa7XO/roo7n//vt7H5AkzSPmYGn6WUD2wFjJ69FHHx13vTVr1rDHHnv0KSpJmh/MwdL0mxNjIAfttNNO43vf+x6HHnooQ0ND7Lrrruy3336sW7eOm2++meOOO44777yTzZs38+Y3v5mTT249r3frGx02bdrEUUcdxYte9CK+9rWvsWTJEj73uc+x8847D/jIJGnmMwdL02/OFZB/+q83cfPdD/Z0mwc9dXfOePnBY7a/+93v5sYbb2TdunVcccUVvOxlL+PGG2/cdqfeOeecw1577cXDDz/MYYcdxqte9Sr23nvvx23j1ltv5bzzzuNDH/oQr371q7nwwgs58cQTe3ocktRv5mBpfphzBeRMsGLFisc95uH9738/F110EQB33nknt9566xOS1/Llyzn00EMBeP7zn8/3v//96QpXkuYUc7DUf3OugBzvW+p0efKTn7xt+oorruDyyy/n6quvZpdddmHlypUdH7a70047bZtesGABDz/88LTEKkm9ZA6W5gdvoumB3XbbjYceeqhj2wMPPMCee+7JLrvswi233MI111wzzdFJ0txmDpam35w7AzkIe++9Ny984Qt59rOfzc4778xTnvKUbW1HHnkkq1ev5pBDDuGZz3wmhx9++AAjlaS5xxwsTT8LyB755Cc/2XH5TjvtxKWXXtqxbesYm0WLFnHjjTduW/62t72t5/FJ0lxmDpamV98uYSc5J8n6JDeO0b4yyQNJ1jWf0/sViyRJknqnn2cgP0rr/asfH6fPV6rqmD7GIEmSpB7r2xnIqroK+FG/ti9JkqTBGPRd2EckuSHJpUkG/+wHSZIkTWiQN9FcDxxQVZuSHA18FjiwU8ckJwMnAzztaU+btgAlSZL0RAM7A1lVD1bVpmZ6DTCUZNEYfc+uquGqGl68ePG0xilJkqTHG1gBmWTfJGmmVzSxbBxUPNNp1113HXQIkjRvmYOlqevbJewk5wErgUVJRoAzgCGAqloNHA+8MckW4GFgVVVVv+KRJElSb/StgKyq10zQfiatx/zMeu94xzs44IAD+N3f/V0A3vWud5GEq666ivvuu49HHnmEv/iLv+DYY48dcKSSNPeYg6XpN/feRHPpaXDPt3u7zX3/Gxz17jGbV61axamnnroteV1wwQVcdtllvOUtb2H33Xfn3nvv5fDDD+cVr3gFzVV7SZqbzMHSvDD3CsgBeO5zn8v69eu5++672bBhA3vuuSf77bcfb3nLW7jqqqvYYYcduOuuu/jhD3/IvvvuO+hwJWlOMQdL02/uFZDjfEvtp+OPP55Pf/rT3HPPPaxatYpzzz2XDRs2cN111zE0NMSyZcvYvHnzQGKTpGljDpbmhblXQA7IqlWreMMb3sC9997LlVdeyQUXXMA+++zD0NAQX/7yl7njjjsGHaIkzVnmYGl6WUD2yMEHH8xDDz3EkiVL2G+//TjhhBN4+ctfzvDwMIceeijPetazBh2iJM1Z5mBpellA9tC3v/2zgeOLFi3i6quv7thv06ZN0xWSJM0b5mBp+gz6XdiSpClIcmSS7ya5LclpHdpXJnkgybrmc/qo9gVJvpnkkumLWtJs5xlISZqlkiwAzgJeAowA1ya5uKpuHtX1K1V1zBibeTPwHWD3/kUqaa7xDKQkzV4rgNuq6vaq+ilwPjDpp2UnWQq8DPjnPsUnaY6aMwXkfHkL4nw5TkmTsgS4s21+pFk22hFJbkhyaZKD25a/F3g78Nh4O0lycpK1SdZu2LChY5/5kJvmwzFKkzUnCsiFCxeycePGOf+Pu6rYuHEjCxcuHHQokmaGTq9VGZ0IrwcOqKrnAB8APguQ5BhgfVVdN9FOqursqhququHFixc/oX0+5GDzr/R4c2IM5NKlSxkZGWGsb8ZzycKFC1m6dOmgw5A0M4wA+7fNLwXubu9QVQ+2Ta9J8sEki4AXAq9IcjSwENg9ySeq6sRug5gvOdj8K/3MnCggh4aGWL58+aDDkKTpdi1wYJLlwF3AKuC17R2S7Av8sKoqyQpaV542VtUfA3/c9FkJvG17ikcwB0vz0ZwoICVpPqqqLUneBHweWACcU1U3JTmlaV8NHA+8MckW4GFgVc3la82SpoUFpCTNYlW1BlgzatnqtukzgTMn2MYVwBV9CE/SHDUnbqKRJEnS9LGAlCRJUlcsICVJktQVC0hJkiR1pW8FZJJzkqxPcuME/Q5L8miS4/sViyRJknqnn2cgPwocOV6HJAuA99B6BIUkSZJmgb4VkFV1FfCjCbr9PnAhsL5fcUiSJKm3BjYGMskS4JXA6on6SpIkaeYY5E007wXeUVWPTtQxyclJ1iZZO9fftSpJkjTTDfJNNMPA+UkAFgFHJ9lSVZ8d3bGqzgbOBhgeHvYVXJIkSQM0sAKyqpZvnU7yUeCSTsWjJEmSZpa+FZBJzgNWAouSjABnAEPw+Pe0SpIkaXbpWwFZVa/pou9v9CsOSZIk9ZZvopEkSVJXLCAlSZLUFQtISZIkdcUCUpIkSV2xgJQkSVJXLCAlSZLUFQtISZIkdcUCUpIkSV2xgJQkSVJXLCAlSZLUFQtISZoBklyY5GVJzMuSZjwTlSTNDP8IvBa4Ncm7kzxr0AFJ0lgsICVpBqiqy6vqBOB5wPeBLyb5WpLfTDI01npJjkzy3SS3JTmtQ/vKJA8kWdd8Tm+WL0zyjSQ3JLkpyZ/269gkzT07DjoASVJLkr2BE4GTgG8C5wIvAl4HrOzQfwFwFvASYAS4NsnFVXXzqK5fqapjRi37CfDLVbWpKVC/muTSqrqml8ckaW6ygJSkGSDJZ4BnAf8CvLyqftA0fSrJ2jFWWwHcVlW3N9s4HzgWGF1APkFVFbCpmR1qPrX9RyBpPvEStiTNDGdW1UFV9VdtxSMAVTU8xjpLgDvb5keaZaMd0VyqvjTJwVsXJlmQZB2wHvhiVX29006SnJxkbZK1GzZs6OaYJM1RFpCSNDP8QpI9ts4k2TPJ706wTjosG30W8XrggKp6DvAB4LPbOlY9WlWHAkuBFUme3WknVXV2VQ1X1fDixYsnPBBJc58FpCTNDG+oqvu3zlTVfcAbJlhnBNi/bX4pcHd7h6p6sKo2NdNrgKEki0b1uR+4AjhyO2OXNM9YQErSzLBDkm1nFJsbZJ40wTrXAgcmWZ7kScAq4OL2Dkn23brdJCto5f2NSRZvPeOZZGfgV4FbenUwkuY2b6KRpJnh88AFSVbTugx9CnDZeCtU1ZYkb2rWXQCcU1U3JTmlaV8NHA+8MckW4GFgVVVVkv2AjzWF6g7ABVV1Sb8OTtLc0rcCMsk5wDHA+qp6wriaJMcCfw48BmwBTq2qr/YrHkma4d4B/A7wRlpjG78A/PNEKzWXpdeMWra6bfpM4MwO630LeO7UQpY0X/XzDORHaSWtj4/R/iXg4uab8CHABbQeYSFJ805VPUbrbTT/OOhYJGkifSsgq+qqJMvGad/UNvtkfP6YpHksyYHAXwEHAQu3Lq+qpw8sKEkaw6Ruokny5iS7p+XDSa5P8tKp7jzJK5PcAvw/4Lemuj1JmsU+Quvs4xbgxbSu3vzLQCOSpDFM9i7s36qqB4GXAouB3wTePdWdV9VFVfUs4Dha4yE78iG2kuaBnavqS0Cq6o6qehfwywOOSZI6mmwBufXREkcDH6mqG+j8ANvtUlVXAc8Y/WyytnYfYitprtucZAfg1iRvSvJKYJ9BByVJnUy2gLwuyRdoFZCfT7Ibrbunt1uSn297NtnzaD3vbONUtilJs9ipwC7AHwDPB04EXjfIgCRpLJO9iea3gUOB26vqx0n2onUZe0xJzgNWAouSjABnAEOw7RETrwJ+PckjtJ5N9mtV5Y00kuad5lmMr66qPwI2MUF+laRBm2wBeQSwrqr+K8mJwPOA9423QlW9ZoL29wDvmeT+JWnOqqpHkzw/SfwiLWk2mOwl7H8EfpzkOcDbgTsY+/mOkqTufRP4XJKTkvzPrZ9BByVJnUz2DOSW5oHfxwLvq6oPJ3FsjiT1zl60xoG333ldwGcGE44kjW2yBeRDSf4YOAn4xWa8zlD/wpKk+aWqHPcoadaYbAH5a8BraT0P8p4kTwP+pn9hSdL8kuQjdHgjV1X5kgVJM86kCsimaDwXOCzJMcA3qsoxkJLUO5e0TS8EXgncPaBYJGlckyogk7ya1hnHK2g9QPwDSf6oqj7dx9gkad6oqgvb55tHoV0+oHAkaVyTvYT9TuCwqloPkGQxrcRmASlJ/XEg8LRBByFJnUy2gNxha/HY2MjkHwEkSZpAkod4/BjIe4B3DCgcSRrXZAvIy5J8Hjivmf81YE1/QpKk+aeqdht0DJI0WZM6i9i8Xuts4BDgOcDZVeU3Y0nqkSSvTPJzbfN7JDlugCFJ0pgmewZy6wDvCyfsKEnaHmdU1UVbZ6rq/iRnAJ8dXEiS1Nm4BWSHMTnbmoCqqt37EpUkzT+drghN+ku+JE2ncZOTY3IkadqsTfL3wFm0vrj/PnDdYEOSpM68k1qSZobfB34KfAq4AHgY+L2BRiRJY/DyiCTNAFX1X8Bpg45DkibDM5CSNAMk+WKSPdrm92wenyZJM44FpCTNDIuq6v6tM1V1H7DP4MKRpLFZQErSzPBYkm2vLkyyjM5PwXicJEcm+W6S25I84RJ4kpVJHkiyrvmc3izfP8mXk3wnyU1J3tzLg5E0tzkGUpJmhncCX01yZTP/P4CTx1shyQJad22/BBgBrk1ycVXdPKrrV6rqmFHLtgBvrarrk+wGXJfkix3WlaQn8AykJM0AVXUZMAx8l9ad2G+ldSf2eFYAt1XV7VX1U+B84NhJ7u8HVXV9M/0Q8B1gyXaGL2me6VsBmeScJOuT3DhG+wlJvtV8vpbkOf2KRZJmuiSvB75Eq3B8K/AvwLsmWG0JcGfb/Aidi8AjktyQ5NIkB3fY9zLgucDXx4jt5CRrk6zdsGHDRIciaR7o5xnIjwJHjtP+n8AvVdUhwJ/Tete2JM1XbwYOA+6oqhfTKugmqtbSYdnocZPXAwdU1XOADzDq1YhJdqX1mtpTq+rBTjupqrOrariqhhcvXjzhgUia+/pWQFbVVcCPxmn/WnOXIcA1wNJ+xSJJs8DmqtoMkGSnqroFeOYE64wA+7fNLwXubu9QVQ9W1aZmeg0wlGRRs58hWsXjuVX1md4chqT5YKaMgfxt4NKxGr18ImkeGGmeA/lZ4ItJPseoYrCDa4EDkyxP8iRgFXBxe4ck+yZJM72CVt7f2Cz7MPCdqvr7nh6JpDlv4HdhJ3kxrQLyRWP1qaqzaS5xDw8PT/hYC0mabarqlc3ku5J8Gfg54LIJ1tmS5E3A54EFwDlVdVOSU5r21cDxwBuTbKF1U86qqqokLwJOAr6dZF2zyT9pzlJK0rgGWkAmOQT4Z+Coqto4yFgkaaaoqisn7rWt7xpgzahlq9umzwTO7LDeV+k8hlKSJjSwS9jNA3M/A5xUVf8xqDgkSZLUnb6dgUxyHrASWJRkBDgDGIJt345PB/YGPtgMz9lSVcP9ikeSJEm90bcCsqpeM0H764HX92v/kiRJ6o+Zche2JEmSZgkLSEmSJHXFAlKSJEldsYCUJElSVywgJUmS1BULSEmSJHXFAlKSJEldsYCUJElSVywgJUmS1BULSEmSJHXFAlKSJEldsYCUJElSVywgJUmS1BULSEmSJHXFAlKSJEldsYCUJElSVywgJUmS1BULSEmSJHXFAlKSJEld6VsBmeScJOuT3DhG+7OSXJ3kJ0ne1q84JEmS1Fv9PAP5UeDIcdp/BPwB8Ld9jEGSJEk91rcCsqquolUkjtW+vqquBR7pVwySJEnqPcdAStIsluTIJN9NcluS0zq0r0zyQJJ1zef0trZxhxpJ0lhmRQGZ5OQka5Os3bBhw6DDkaQZIckC4CzgKOAg4DVJDurQ9StVdWjz+bO25R9l/KFGktTRrCggq+rsqhququHFixcPOhxJmilWALdV1e1V9VPgfODYya480VAjSRrLrCggJUkdLQHubJsfaZaNdkSSG5JcmuTgbnfiVSBJo+3Yrw0nOQ9YCSxKMgKcAQwBVNXqJPsCa4HdgceSnAocVFUP9ismSZpj0mFZjZq/HjigqjYlORr4LHBgNzupqrOBswGGh4dHb1/SPNS3ArKqXjNB+z3A0n7tX5LmgRFg/7b5pcDd7R3av5RX1ZokH0yyqKrunaYYJc1BXsKWpNnrWuDAJMuTPAlYBVzc3iHJvknSTK+glfc3TnukkuYUC0hJmqWqagvwJuDzwHeAC6rqpiSnJDml6XY8cGOSG4D3A6uqqmDbUKOrgWcmGUny29N/FJJmo75dwpYk9V9VrQHWjFq2um36TODMMdYdd6iRJI3FM5CSJEnqigWkJEmSumIBKUmSpK5YQEqSJKkrFpCSJEnqigWkJEmSumIBKUmSpK5YQEqSJKkrFpCSJEnqigWkJEmSumIBKUmSpK5YQEqSJKkrFpCSJEnqigWkJEmSumIBKUmSpK5YQEqSJKkrFpCSJEnqSt8KyCTnJFmf5MYx2pPk/UluS/KtJM/rVyySJEnqnR37uO2PAmcCHx+j/SjgwObzAuAfm5/S411xxaAjkKT5yxysDvp2BrKqrgJ+NE6XY4GPV8s1wB5J9utXPJIkSeqNQY6BXALc2TY/0iyTJEnSDDbIAjIdllXHjsnJSdYmWbthw4Y+hyVJkqTxDLKAHAH2b5tfCtzdqWNVnV1Vw1U1vHjx4mkJTpIkSZ0NsoC8GPj15m7sw4EHquoHA4xHkiRJk9C3u7CTnAesBBYlGQHOAIYAqmo1sAY4GrgN+DHwm/2KRZIkSb3TtwKyql4zQXsBv9ev/UuSJKk/0qrjZo8kG4A7Bh1HYxFw76CD6NJsjBmMe7rNlLgPqCoHPs8gMygHz5S/0W4Z9/Qy7qkZMwfPugJyJkmytqqGBx1HN2ZjzGDc0222xq35Y7b+jRr39DLu/vFd2JIkSeqKBaQkSZK6YgE5NWcPOoDtMBtjBuOebrM1bs0fs/Vv1Linl3H3iWMgJUmS1BXPQEqSJKkrFpDjSLJXki8mubX5uecY/Y5M8t0ktyU5rUP725JUkkX9j3rqcSf5myS3JPlWkouS7NHneCf6/SXJ+5v2byV53mTXnWkxJ9k/yZeTfCfJTUnePF0xTyXutvYFSb6Z5JLpi1rzlTm4/zl4NubfqcRtDu6hqvIzxgf4a+C0Zvo04D0d+iwAvgc8HXgScANwUFv7/sDnaT03bdFsiBt4KbBjM/2eTuv3MNZxf39Nn6OBS4EAhwNfn+y6MzDm/YDnNdO7Af8xHTFPNe629j8EPglcMh0x+5nfH3Nwf3PwbMy/PYjbHNyjj2cgx3cs8LFm+mPAcR36rABuq6rbq+qnwPnNelv9A/B2YDoHm04p7qr6QlVtafpdAyztY6wT/f5o5j9eLdcAeyTZb5LrzqiYq+oHVXU9QFU9BHwHWDINMU8pboAkS4GXAf88TfFK5uD+5uDZmH+nFLc5uHcsIMf3lKr6AUDzc58OfZYAd7bNjzTLSPIK4K6quqHfgY4ypbhH+S1a34b6ZTJxjNVnssfQa1OJeZsky4DnAl/vfYgdTTXu99L6j/ixPsUnjWYO7m8Ono35d7yYuupjDp6avr0Le7ZIcjmwb4emd052Ex2WVZJdmm28dHtjG3enfYp71D7eCWwBzu0uuq5MGMc4fSazbj9MJeZWY7IrcCFwalU92MPYxrPdcSc5BlhfVdclWdnrwDR/mYMfZ7pz8GzMv0xy3+bgPpv3BWRV/epYbUl+uPWUd3MKeX2HbiO0xthstRS4G3gGsBy4IcnW5dcnWVFV98zguLdu43XAMcCvVDPwok/GjWOCPk+axLr9MJWYSTJEK3GdW1Wf6WOco00l7uOBVyQ5GlgI7J7kE1V1Yh/j1TxgDn5C3Fu3MR05eDbm3/FimlQfc3CPDHoQ5kz+AH/D4wdC/3WHPjsCt9NKVFsHxR7cod/3mb4B3FOKGzgSuBlYPA2xTvj7ozXmo31Q8Te6+d3PsJgDfBx47wD+nrc77lF9VjIDBnD7mfsfc3B/c/BszL89iNsc3KvjGXQAM/kD7A18Cbi1+blXs/ypwJq2fkfTupPre8A7x9jWdCavKcUN3EZrDMa65rO6z/E+IQ7gFOCUZjrAWU37t4Hhbn73Mylm4EW0Lll8q+33e/RMj3vUNmZE8vIz9z/m4P7n4NmYf6cStzm4dx/fRCNJkqSueBe2JEmSumIBKUmSpK5YQEqSJKkrFpCSJEnqigWkJEmSumIBqTkjycoklww6Dkmab8y/848FpCRJkrpiAalpl+TEJN9Isi7JPyVZkGRTkr9Lcn2SLyVZ3PQ9NMk1Sb6V5KIkezbLfz7J5UluaNZ5RrP5XZN8OsktSc5N8w4zSZL5V71jAalpleQXgF8DXlhVhwKPAicATwaur6rnAVcCZzSrfBx4R1UdQuup/FuXnwucVVXPAf478INm+XOBU4GDgKcDL+zzIUnSrGD+VS/tOOgANO/8CvB84Nrmy+nOwHrgMeBTTZ9PAJ9J8nPAHlV1ZbP8Y8D/TbIbsKSqLgKoqs0Azfa+UVUjzfw6YBnw1b4flSTNfOZf9YwFpKZbgI9V1R8/bmHyf0b1G+8dm+NdFvlJ2/Sj+DcuSVuZf9UzXsLWdPsScHySfQCS7JXkAFp/i8c3fV4LfLWqHgDuS/KLzfKTgCur6kFgJMlxzTZ2SrLLdB6EJM1C5l/1jN8ONK2q6uYk/xv4QpIdgEeA3wP+Czg4yXXAA7TG6QC8DljdJKjbgd9slp8E/FOSP2u28b+m8TAkadYx/6qXUjXemWppeiTZVFW7DjoOSZpvzL/aHl7CliRJUlc8AylJkqSueAZSkiRJXbGAlCRJUlcsICVJktQVC0hJkiR1xQJSkiRJXbGAlCRJUlf+P9O4JSWI/ZnzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train model A\n",
    "modelA = TransformerClassifier(embedding=embedding,\n",
    "                              src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 64,\n",
    "                              ffn_hidden = 512,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "optA = torch.optim.Adam(modelA.parameters(), lr=0.001)\n",
    "loss_fnA = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "historyA, best_modelA, best_model_scoreA = train_save_best(model=modelA,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optA,\n",
    "                                                            criterion=loss_fnA,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# # save model\n",
    "# torch.save(best_modelA, './Models/modelA')\n",
    "\n",
    "# # save history\n",
    "# with open('./Models/historyA.txt', 'w') as dat:\n",
    "#     dat.write(str(historyA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model B\n",
    "modelB = TransformerClassifier(embedding=embedding,\n",
    "                              src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "optB = torch.optim.SGD(modelB.parameters(), lr=0.001)\n",
    "loss_fnB = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "historyB, best_modelB, best_model_scoreB = train_save_best(model=modelB,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optB,\n",
    "                                                            criterion=loss_fnB,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# save model\n",
    "torch.save(best_modelB, './Models/modelB')\n",
    "\n",
    "# save history\n",
    "with open('./Models/historyB.txt', 'w') as dat:\n",
    "    dat.write(str(historyB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weight_averaging(*weights):\n",
    "  with torch.no_grad():\n",
    "    sum = torch.zeros(weights[0].shape, device=device)\n",
    "    for weight in weights:\n",
    "      sum += weight\n",
    "  return sum / len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_averaging(*linears):\n",
    "  \"\"\"Averages several linear layers (weights + biases)\"\"\"\n",
    "  with torch.no_grad():\n",
    "    weights = [linear.weight for linear in linears]\n",
    "    biases = [linear.bias for linear in linears]\n",
    "\n",
    "    linear_averaged = torch.nn.Linear(linears[0].in_features, linears[0].out_features, bias=True).to(device)\n",
    "    linear_averaged.weight, linear_averaged.bias = torch.nn.Parameter(weight_averaging(*weights)), torch.nn.Parameter(weight_averaging(*biases))\n",
    "\n",
    "  return linear_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vanilla_fusion(modelA, modelB):\n",
    "  # init\n",
    "  model_fusion = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # 1) encoder\n",
    "    # TODO: smarter method for embedding\n",
    "    # a) embedding\n",
    "    weights_A = modelA.encoder.emb.tok_emb.weight\n",
    "    weights_B = modelB.encoder.emb.tok_emb.weight\n",
    "\n",
    "    weights_fusion = weight_averaging(weights_A, weights_B) # weights seem to be exactly the same?\n",
    "    model_fusion.encoder.emb.tok_emb.weight = torch.nn.Parameter(weights_fusion)\n",
    "\n",
    "    # b) encoder layers\n",
    "    for i, _ in enumerate(modelA.encoder.layers):\n",
    "      # i) self-attention (fuse Q, K, V separately) # TODO: check validity of this approach\n",
    "      # query\n",
    "      query_A = modelA.encoder.layers[i].attention.w_q\n",
    "      query_B = modelB.encoder.layers[i].attention.w_q\n",
    "\n",
    "      query_fusion = linear_averaging(query_A, query_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_q = query_fusion\n",
    "\n",
    "      # key\n",
    "      key_A = modelA.encoder.layers[i].attention.w_k\n",
    "      key_B = modelB.encoder.layers[i].attention.w_k\n",
    "\n",
    "      key_fusion = linear_averaging(key_A, key_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_k = key_fusion\n",
    "\n",
    "      # value\n",
    "      value_A = modelA.encoder.layers[i].attention.w_v\n",
    "      value_B = modelB.encoder.layers[i].attention.w_v\n",
    "\n",
    "      value_fusion = linear_averaging(value_A, value_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_v = value_fusion\n",
    "\n",
    "      # output\n",
    "      output_A = modelA.encoder.layers[i].attention.w_concat\n",
    "      output_B = modelB.encoder.layers[i].attention.w_concat\n",
    "\n",
    "      output_fusion = linear_averaging(output_A, output_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_concat = output_fusion\n",
    "\n",
    "      # ii) layer norm 1\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "      # iii) feed-forward network\n",
    "      # layer 1\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear1\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear1\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear1 = linear_fusion\n",
    "\n",
    "      # layer 2\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear2\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear2\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear2 = linear_fusion\n",
    "\n",
    "      # iv) layer norm 2\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "    # 2) MLP head\n",
    "    linear_A = modelA.linear\n",
    "    linear_B = modelB.linear\n",
    "\n",
    "    linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "    model_fusion.linear = linear_fusion\n",
    "\n",
    "  return model_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_fusion(modelA, modelB, model_fusion):\n",
    "  # test fusion\n",
    "  test_loss_A, test_acc_A = validation(modelA, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_B, test_acc_B = validation(modelB, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_fusion, test_acc_fusion = validation(model_fusion, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "  # visualize\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  metrics_A = [test_loss_A, test_acc_A]\n",
    "  metrics_B = [test_loss_B, test_acc_B]\n",
    "  metrics_fusion = [test_loss_fusion, test_acc_fusion]\n",
    "  metrics = ['loss', 'accuracy']\n",
    "  x = np.arange(len(metrics)) # positions of bars (1 per metric)\n",
    "  width = 0.25  # the width of the bars\n",
    "\n",
    "  rects1 = ax.bar(x - width, metrics_A, width, label='model A')\n",
    "  rects2 = ax.bar(x, metrics_B, width, label='model B')\n",
    "  rects3 = ax.bar(x + width, metrics_fusion, width, label='model fusion')\n",
    "\n",
    "  ax.set_ylabel('Score')\n",
    "  ax.set_title('Test metrics by models')\n",
    "  ax.set_xticks(x)\n",
    "  ax.set_xticklabels(metrics)\n",
    "  ax.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (Optional) load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load parent models\n",
    "# TODO: change enc_voc_size back\n",
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "modelA.load_state_dict(torch.load('./Models/modelA'))\n",
    "modelB.load_state_dict(torch.load('./Models/modelB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vanilla fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion = vanilla_fusion(modelA, modelB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Optimal transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                               enc_voc_size = voc_size,\n",
    "                               max_len = 256,\n",
    "                               d_model = 512,\n",
    "                               ffn_hidden = 2048,\n",
    "                               n_head = 1,\n",
    "                               n_layers = 1,\n",
    "                               drop_prob = 0.1,\n",
    "                               device = device)\n",
    "\n",
    "model_fusion.load_state_dict(torch.load('./Models/model_fusion_OT_pre_retraining'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Randomly initialiized model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with new randomly initialized transformer\n",
    "test_fusion(modelA, modelB, TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'model_fusion_OT_post_retraining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "opt_fusion = torch.optim.SGD(model_fusion.parameters(), lr=0.001)\n",
    "loss_fn_fusion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "history_fusion, best_model_fusion, best_model_score_fusion = train_save_best(model=model_fusion,\n",
    "                                                                             iterator=train_iter,\n",
    "                                                                             optimizer=opt_fusion,\n",
    "                                                                             criterion=loss_fn_fusion,\n",
    "                                                                             epoch=epochs,\n",
    "                                                                             clip=1,\n",
    "                                                                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(best_model_fusion, f'./Models/{model_name}')\n",
    "\n",
    "# save history\n",
    "with open(f'./Models/history_{model_name}.txt', 'w') as dat:\n",
    "    dat.write(str(history_fusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load best model (current model is trained on full epochs)\n",
    "model_fusion.load_state_dict(best_model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion (after retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
