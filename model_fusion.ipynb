{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import spacy\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training(history, marker=None):\n",
    "  plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.01,\n",
    "                    right=1.5,\n",
    "                    top=0.6,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(history['train_loss'])\n",
    "  plt.plot(history['val_loss'])\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training loss')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_loss'] + history['val_loss'])\n",
    "    y_max = max(history['train_loss'] + history['val_loss'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(history['train_acc'])\n",
    "  plt.plot(history['val_acc'])\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.title('Training metric')\n",
    "\n",
    "  # vertical line for marking best epoch\n",
    "  if marker is not None:\n",
    "    y_min = min(history['train_acc'] + history['val_acc'])\n",
    "    y_max = max(history['train_acc'] + history['val_acc'])\n",
    "    plt.vlines(x=marker, ymin=y_min, ymax=y_max, color='red')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, iterator, optimizer, criterion, device):\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # validation\n",
    "    # loss, metrics for current epoch\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # stop graph\n",
    "        # batches\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            output = model(src)\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_accuracy += accuracy\n",
    "\n",
    "    # return mean loss w.r.t. batches\n",
    "    return val_epoch_loss / len(iterator), val_epoch_accuracy / len(iterator)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': []}\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "          for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc]):\n",
    "            history[key].append(value)\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history)\n",
    "\n",
    "    return history\n",
    "\n",
    "def train_save_best(model, iterator, optimizer, criterion, epoch, clip, device):\n",
    "    # set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "    # save data - init\n",
    "    history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc': [],\n",
    "               'val_acc': [],\n",
    "               'learning_rate': []}\n",
    "    best_model = None\n",
    "    best_model_score = 1e9\n",
    "    best_model_epoch = 0\n",
    "\n",
    "    # training\n",
    "    for e in range(epoch):\n",
    "        # loss, metrics for current epoch\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        # batches\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = model(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy / len(iterator)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step() # optimize model\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validation(model, valid_iter, optimizer, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # save data\n",
    "        with torch.no_grad():\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            for key, value in zip(history.keys(), [epoch_loss / len(iterator), val_loss, epoch_acc, val_acc, current_lr]):\n",
    "                history[key].append(value)\n",
    "\n",
    "            # save best model (w.r.t validation loss)\n",
    "            if val_loss < best_model_score:\n",
    "                best_model = model.state_dict()\n",
    "                best_model_score = val_loss\n",
    "                best_model_epoch = e\n",
    "\n",
    "        # visualization\n",
    "        print(f\"Epoch: {e + 1}  Train Loss: {epoch_loss / len(iterator):.4f} \\\n",
    "              Validation Loss: {val_loss:.4f} \\\n",
    "              Train acc: {epoch_acc:.4f}, \\\n",
    "              Val acc: {val_acc:.4f}, \\\n",
    "              Learning Rate : {optimizer.param_groups[0]['lr'] :.4f}\")\n",
    "\n",
    "    # print training curve\n",
    "    plot_training(history, marker=best_model_epoch)\n",
    "\n",
    "    return history, best_model, best_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "Tokenizing the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atace\\OneDrive\\Desktop\\ETH\\9.Semester\\Deep Learning\\project\\Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers\\dataloader.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"len\"] = data.iloc[:, 0].apply(lambda x : len(self.tokenize(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data :  29544\n",
      "0\n",
      "review       [[CLS], one, of, the, many, silent, comedies, ...\n",
      "sentiment                                                    0\n",
      "len                                                        186\n",
      "Name: 46539, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23635/23635 [00:01<00:00, 17130.39it/s]\n",
      "100%|██████████| 2954/2954 [00:00<00:00, 16272.86it/s]\n",
      "100%|██████████| 2955/2955 [00:00<00:00, 17310.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "Vocabulary Size :  23050\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "# import data (combine train/test as we split afterwards anyways)\n",
    "# data = pd.concat([pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1'),\n",
    "#                   pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1')])\n",
    "data = pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1')\n",
    "# convert string label to binary (int) label (spam:1, non-spam:0)\n",
    "data[\"sentiment\"] = data['sentiment'].apply(lambda x : int(x == \"positive\"))\n",
    "\n",
    "# train, test, val split\n",
    "train, valid, test = loader.make_dataset(data)\n",
    "vocab = loader.get_vocab(train.iloc[:, 0])\n",
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=512,\n",
    "                                                     device=device)\n",
    "\n",
    "# NLP stuff\n",
    "pad_idx = vocab['__PAD__']\n",
    "voc_size = len(vocab)\n",
    "print(\"Vocabulary Size : \", voc_size)\n",
    "\n",
    "# Creating the embedding matrix\n",
    "embedding = torch.nn.Embedding(voc_size, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Idea: We train model A and model B for long enough, s.t. they start overfitting. We use their best models w.r.t. validation set (i.e. not the final model after all training epochs) and fuse them together. The fused model is then trained for long enough as well, saving the best model w.r.t to the same validation set. The fused model is then compared with its parent models on the separate test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "note that dataset is imbalanced -> accuracy is not a good metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model A\n",
    "modelA = TransformerClassifier(embedding=embedding,\n",
    "                              src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.5,\n",
    "                              device = device)\n",
    "\n",
    "optA = torch.optim.Adam(modelA.parameters(), lr=0.0002)\n",
    "loss_fnA = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "historyA, best_modelA, best_model_scoreA = train_save_best(model=modelA,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optA,\n",
    "                                                            criterion=loss_fnA,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# save model\n",
    "torch.save(best_modelA, './Models/modelA_IMDB_256')\n",
    "\n",
    "# save history\n",
    "with open('./Models/historyA.txt', 'w') as dat:\n",
    "    dat.write(str(historyA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_24316/3037923790.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 47/47 [02:25<00:00,  3.09s/it]\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_24316/3037923790.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Train Loss: 0.8118               Validation Loss: 0.5441               Train acc: 0.5477,               Val acc: 0.7231,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:48<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  Train Loss: 0.5001               Validation Loss: 0.5396               Train acc: 0.7518,               Val acc: 0.7460,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:46<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3  Train Loss: 0.3916               Validation Loss: 0.4054               Train acc: 0.8260,               Val acc: 0.8248,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:42<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4  Train Loss: 0.3138               Validation Loss: 0.3849               Train acc: 0.8655,               Val acc: 0.8391,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:50<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5  Train Loss: 0.2790               Validation Loss: 0.3823               Train acc: 0.8827,               Val acc: 0.8392,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:40<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6  Train Loss: 0.2282               Validation Loss: 0.4348               Train acc: 0.9078,               Val acc: 0.8353,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:34<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7  Train Loss: 0.2355               Validation Loss: 0.3778               Train acc: 0.9020,               Val acc: 0.8613,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:39<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8  Train Loss: 0.1821               Validation Loss: 0.4503               Train acc: 0.9296,               Val acc: 0.8399,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:41<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Train Loss: 0.1685               Validation Loss: 0.4466               Train acc: 0.9344,               Val acc: 0.8421,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:39<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10  Train Loss: 0.1357               Validation Loss: 0.4605               Train acc: 0.9496,               Val acc: 0.8425,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:34<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11  Train Loss: 0.1117               Validation Loss: 0.5454               Train acc: 0.9610,               Val acc: 0.8276,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12  Train Loss: 0.1375               Validation Loss: 0.4444               Train acc: 0.9442,               Val acc: 0.8695,               Learning Rate : 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:37<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13  Train Loss: 0.0883               Validation Loss: 0.4824               Train acc: 0.9706,               Val acc: 0.8569,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:41<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14  Train Loss: 0.0620               Validation Loss: 0.4608               Train acc: 0.9822,               Val acc: 0.8663,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:40<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15  Train Loss: 0.0558               Validation Loss: 0.4686               Train acc: 0.9857,               Val acc: 0.8677,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:43<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16  Train Loss: 0.0539               Validation Loss: 0.4721               Train acc: 0.9863,               Val acc: 0.8679,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:29<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17  Train Loss: 0.0523               Validation Loss: 0.4770               Train acc: 0.9874,               Val acc: 0.8656,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:29<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18  Train Loss: 0.0505               Validation Loss: 0.4792               Train acc: 0.9876,               Val acc: 0.8675,               Learning Rate : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:46<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19  Train Loss: 0.0479               Validation Loss: 0.4843               Train acc: 0.9885,               Val acc: 0.8669,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:30<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20  Train Loss: 0.0463               Validation Loss: 0.4891               Train acc: 0.9895,               Val acc: 0.8653,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:35<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21  Train Loss: 0.0453               Validation Loss: 0.4880               Train acc: 0.9897,               Val acc: 0.8651,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:29<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22  Train Loss: 0.0460               Validation Loss: 0.4878               Train acc: 0.9895,               Val acc: 0.8645,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:59<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23  Train Loss: 0.0452               Validation Loss: 0.4871               Train acc: 0.9901,               Val acc: 0.8659,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:25<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24  Train Loss: 0.0449               Validation Loss: 0.4868               Train acc: 0.9900,               Val acc: 0.8649,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:25<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25  Train Loss: 0.0447               Validation Loss: 0.4929               Train acc: 0.9900,               Val acc: 0.8637,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:26<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26  Train Loss: 0.0447               Validation Loss: 0.4877               Train acc: 0.9901,               Val acc: 0.8653,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:28<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27  Train Loss: 0.0447               Validation Loss: 0.4853               Train acc: 0.9901,               Val acc: 0.8653,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:44<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28  Train Loss: 0.0452               Validation Loss: 0.4896               Train acc: 0.9899,               Val acc: 0.8643,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:40<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29  Train Loss: 0.0443               Validation Loss: 0.4889               Train acc: 0.9901,               Val acc: 0.8649,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:38<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30  Train Loss: 0.0444               Validation Loss: 0.4878               Train acc: 0.9900,               Val acc: 0.8653,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:40<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31  Train Loss: 0.0444               Validation Loss: 0.4915               Train acc: 0.9901,               Val acc: 0.8639,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:38<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32  Train Loss: 0.0444               Validation Loss: 0.4879               Train acc: 0.9901,               Val acc: 0.8654,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:42<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33  Train Loss: 0.0447               Validation Loss: 0.4908               Train acc: 0.9899,               Val acc: 0.8641,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:40<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34  Train Loss: 0.0442               Validation Loss: 0.4905               Train acc: 0.9901,               Val acc: 0.8646,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:42<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35  Train Loss: 0.0444               Validation Loss: 0.4885               Train acc: 0.9899,               Val acc: 0.8649,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:51<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36  Train Loss: 0.0446               Validation Loss: 0.4914               Train acc: 0.9899,               Val acc: 0.8642,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:38<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37  Train Loss: 0.0446               Validation Loss: 0.4895               Train acc: 0.9899,               Val acc: 0.8643,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:46<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38  Train Loss: 0.0446               Validation Loss: 0.4898               Train acc: 0.9901,               Val acc: 0.8646,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:44<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39  Train Loss: 0.0450               Validation Loss: 0.4910               Train acc: 0.9894,               Val acc: 0.8647,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:45<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40  Train Loss: 0.0459               Validation Loss: 0.4874               Train acc: 0.9899,               Val acc: 0.8654,               Learning Rate : 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 10min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train model B\n",
    "modelB = TransformerClassifier(embedding=embedding,\n",
    "                              src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.5,\n",
    "                              device = device)\n",
    "\n",
    "optB = torch.optim.Adam(modelB.parameters(), lr=0.002)\n",
    "loss_fnB = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "historyB, best_modelB, best_model_scoreB = train_save_best(model=modelB,\n",
    "                                                            iterator=train_iter,\n",
    "                                                            optimizer=optB,\n",
    "                                                            criterion=loss_fnB,\n",
    "                                                            epoch=epochs,\n",
    "                                                            clip=1,\n",
    "                                                            device=device)\n",
    "\n",
    "# save model\n",
    "torch.save(best_modelB, './Models/modelB_IMBD_256')\n",
    "\n",
    "# save history\n",
    "with open('./Models/historyB_IMDB.txt', 'w') as dat:\n",
    "    dat.write(str(historyB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weight_averaging(*weights):\n",
    "  with torch.no_grad():\n",
    "    sum = torch.zeros(weights[0].shape, device=device)\n",
    "    for weight in weights:\n",
    "      sum += weight\n",
    "  return sum / len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_averaging(*linears):\n",
    "  \"\"\"Averages several linear layers (weights + biases)\"\"\"\n",
    "  with torch.no_grad():\n",
    "    weights = [linear.weight for linear in linears]\n",
    "    biases = [linear.bias for linear in linears]\n",
    "\n",
    "    linear_averaged = torch.nn.Linear(linears[0].in_features, linears[0].out_features, bias=True).to(device)\n",
    "    linear_averaged.weight, linear_averaged.bias = torch.nn.Parameter(weight_averaging(*weights)), torch.nn.Parameter(weight_averaging(*biases))\n",
    "\n",
    "  return linear_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vanilla_fusion(modelA, modelB):\n",
    "  # init\n",
    "  model_fusion = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # 1) encoder\n",
    "    # TODO: smarter method for embedding\n",
    "    # a) embedding\n",
    "    weights_A = modelA.encoder.emb.tok_emb.weight\n",
    "    weights_B = modelB.encoder.emb.tok_emb.weight\n",
    "\n",
    "    weights_fusion = weight_averaging(weights_A, weights_B) # weights seem to be exactly the same?\n",
    "    model_fusion.encoder.emb.tok_emb.weight = torch.nn.Parameter(weights_fusion)\n",
    "\n",
    "    # b) encoder layers\n",
    "    for i, _ in enumerate(modelA.encoder.layers):\n",
    "      # i) self-attention (fuse Q, K, V separately) # TODO: check validity of this approach\n",
    "      # query\n",
    "      query_A = modelA.encoder.layers[i].attention.w_q\n",
    "      query_B = modelB.encoder.layers[i].attention.w_q\n",
    "\n",
    "      query_fusion = linear_averaging(query_A, query_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_q = query_fusion\n",
    "\n",
    "      # key\n",
    "      key_A = modelA.encoder.layers[i].attention.w_k\n",
    "      key_B = modelB.encoder.layers[i].attention.w_k\n",
    "\n",
    "      key_fusion = linear_averaging(key_A, key_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_k = key_fusion\n",
    "\n",
    "      # value\n",
    "      value_A = modelA.encoder.layers[i].attention.w_v\n",
    "      value_B = modelB.encoder.layers[i].attention.w_v\n",
    "\n",
    "      value_fusion = linear_averaging(value_A, value_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_v = value_fusion\n",
    "\n",
    "      # output\n",
    "      output_A = modelA.encoder.layers[i].attention.w_concat\n",
    "      output_B = modelB.encoder.layers[i].attention.w_concat\n",
    "\n",
    "      output_fusion = linear_averaging(output_A, output_B)\n",
    "      model_fusion.encoder.layers[i].attention.w_concat = output_fusion\n",
    "\n",
    "      # ii) layer norm 1\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "      # iii) feed-forward network\n",
    "      # layer 1\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear1\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear1\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear1 = linear_fusion\n",
    "\n",
    "      # layer 2\n",
    "      linear_A = modelA.encoder.layers[i].ffn.linear2\n",
    "      linear_B = modelB.encoder.layers[i].ffn.linear2\n",
    "\n",
    "      linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "      model_fusion.encoder.layers[i].ffn.linear2 = linear_fusion\n",
    "\n",
    "      # iv) layer norm 2\n",
    "      # TODO: LAYER NORM WEIGHTS ARE NOT CALLABLE???\n",
    "\n",
    "    # 2) MLP head\n",
    "    linear_A = modelA.linear\n",
    "    linear_B = modelB.linear\n",
    "\n",
    "    linear_fusion = linear_averaging(linear_A, linear_B)\n",
    "    model_fusion.linear = linear_fusion\n",
    "\n",
    "  return model_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_fusion(modelA, modelB, model_fusion):\n",
    "  # test fusion\n",
    "  test_loss_A, test_acc_A = validation(modelA, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_B, test_acc_B = validation(modelB, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "  test_loss_fusion, test_acc_fusion = validation(model_fusion, test_iter, None, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "  # visualize\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  metrics_A = [test_loss_A, test_acc_A]\n",
    "  metrics_B = [test_loss_B, test_acc_B]\n",
    "  metrics_fusion = [test_loss_fusion, test_acc_fusion]\n",
    "  metrics = ['loss', 'accuracy']\n",
    "  x = np.arange(len(metrics)) # positions of bars (1 per metric)\n",
    "  width = 0.25  # the width of the bars\n",
    "\n",
    "  rects1 = ax.bar(x - width, metrics_A, width, label='model A')\n",
    "  rects2 = ax.bar(x, metrics_B, width, label='model B')\n",
    "  rects3 = ax.bar(x + width, metrics_fusion, width, label='model fusion')\n",
    "\n",
    "  ax.set_ylabel('Score')\n",
    "  ax.set_title('Test metrics by models')\n",
    "  ax.set_xticks(x)\n",
    "  ax.set_xticklabels(metrics)\n",
    "  ax.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### (Optional) load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load parent models\n",
    "# TODO: change enc_voc_size back\n",
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device)\n",
    "\n",
    "modelA.load_state_dict(torch.load('./Models/modelA'))\n",
    "modelB.load_state_dict(torch.load('./Models/modelB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vanilla fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion = vanilla_fusion(modelA, modelB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Optimal transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fusion = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                               enc_voc_size = voc_size,\n",
    "                               max_len = 256,\n",
    "                               d_model = 512,\n",
    "                               ffn_hidden = 2048,\n",
    "                               n_head = 1,\n",
    "                               n_layers = 1,\n",
    "                               drop_prob = 0.1,\n",
    "                               device = device)\n",
    "\n",
    "model_fusion.load_state_dict(torch.load('./Models/model_fusion_OT_pre_retraining'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Randomly initialiized model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with new randomly initialized transformer\n",
    "test_fusion(modelA, modelB, TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 512,\n",
    "                              ffn_hidden = 2048,\n",
    "                              n_head = 1,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.1,\n",
    "                              device = device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'model_fusion_OT_post_retraining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "opt_fusion = torch.optim.SGD(model_fusion.parameters(), lr=0.001)\n",
    "loss_fn_fusion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "history_fusion, best_model_fusion, best_model_score_fusion = train_save_best(model=model_fusion,\n",
    "                                                                             iterator=train_iter,\n",
    "                                                                             optimizer=opt_fusion,\n",
    "                                                                             criterion=loss_fn_fusion,\n",
    "                                                                             epoch=epochs,\n",
    "                                                                             clip=1,\n",
    "                                                                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(best_model_fusion, f'./Models/{model_name}')\n",
    "\n",
    "# save history\n",
    "with open(f'./Models/history_{model_name}.txt', 'w') as dat:\n",
    "    dat.write(str(history_fusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load best model (current model is trained on full epochs)\n",
    "model_fusion.load_state_dict(best_model_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test fusion (after retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test with vanilla fusion\n",
    "test_fusion(modelA, modelB, model_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
