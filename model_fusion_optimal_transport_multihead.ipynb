{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerClassifier\n",
    "from dataloader import *\n",
    "import torch\n",
    "import ot\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "Length of data after first step of preprocessing:  35832\n",
      "Tokenizing the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atace\\OneDrive\\Desktop\\ETH\\9.Semester\\Deep Learning\\project\\Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers\\dataloader.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"len\"] = data.iloc[:, 0].apply(lambda x : len(self.tokenize(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data :  29544\n",
      "0\n",
      "review       [[CLS], one, of, the, many, silent, comedies, ...\n",
      "sentiment                                                    0\n",
      "len                                                        186\n",
      "Name: 46539, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23635/23635 [00:01<00:00, 18002.16it/s]\n",
      "100%|██████████| 2954/2954 [00:00<00:00, 16535.46it/s]\n",
      "100%|██████████| 2955/2955 [00:00<00:00, 17959.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "Vocabulary Size :  23050\n"
     ]
    }
   ],
   "source": [
    "#init\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoader(tokenize = tokenizer.tokenize)\n",
    "\n",
    "# import data (combine train/test as we split afterwards anyways)\n",
    "data = pd.read_csv(\"./Data/IMDB Dataset.csv\", encoding='ISO-8859-1')\n",
    "# convert string label to binary (int) label (spam:1, non-spam:0)\n",
    "data[\"sentiment\"] = data['sentiment'].apply(lambda x : int(x == \"positive\"))\n",
    "\n",
    "# train, test, val split\n",
    "train, valid, test = loader.make_dataset(data)\n",
    "vocab = loader.get_vocab(train.iloc[:, 0])\n",
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=512,\n",
    "                                                     device=device)\n",
    "\n",
    "# NLP stuff\n",
    "pad_idx = vocab['__PAD__']\n",
    "voc_size = len(vocab)\n",
    "print(\"Vocabulary Size : \", voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, iterator, criterion, device):\n",
    "    # set model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # validation\n",
    "    # loss, metrics for current epoch\n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # stop graph\n",
    "        # batches\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            output = model(src)\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_accuracy += accuracy\n",
    "\n",
    "    # return mean loss w.r.t. batches\n",
    "    return val_epoch_loss / len(iterator), val_epoch_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding matrix\n",
    "embedding = torch.load(\"Models/embedding_16_trained_multihead.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(\n",
       "        (embedding): Embedding(23050, 16)\n",
       "      )\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_k): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_v): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_concat): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding=embedding,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 4,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.5,\n",
    "                              device = device)\n",
    "\n",
    "modelA.load_state_dict(torch.load(\"Models\\modelA_IMDB_256_multihead\"))\n",
    "modelA.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(\n",
       "        (embedding): Embedding(23050, 16)\n",
       "      )\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_k): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_v): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (w_concat): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding=embedding,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 4,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.5,\n",
    "                              device = device)\n",
    "\n",
    "modelB.load_state_dict(torch.load(\"Models\\modelB_IMDB_256_multihead\"))\n",
    "modelB.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OT Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSupport(model, trainloader, l, alignment = \"acts\", numOfBatches= 10):\n",
    "    '''\n",
    "    Get the support matrices using Activation-based (\"acts\") or Weight-based (\"wts\") alignment \n",
    "    '''\n",
    "    if alignment == \"acts\":\n",
    "        activation = None\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if i >= numOfBatches:\n",
    "                break\n",
    "            \n",
    "            inputs, targets = data\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if activation is None:\n",
    "                activation = model.actMatrix[l]\n",
    "            else:\n",
    "                activation = torch.cat((activation, model.actMatrix[l]))\n",
    "\n",
    "        return activation\n",
    "    elif alignment == \"wts\":\n",
    "        return model.state_dict()[l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion(nameA, nameB, weightA, weightB, transport_matrix, beta):\n",
    "    support_y = getSupport(modelB, train_iter, nameB, alignment=\"wts\")\n",
    "    # Get the weights at layer \"idx\" from the first model\n",
    "    W_A = weightA\n",
    "    W_B = weightB\n",
    "    # Align the weights from the first model\n",
    "    aligned_W = torch.matmul(W_A, torch.matmul(transport_matrix, torch.diag(1 / beta)))\n",
    "    # Get the X-Support\n",
    "    n = W_A.shape[0]\n",
    "    alpha = torch.ones(n) * (1/n)\n",
    "    support_x = getSupport(modelA, train_iter, nameA, alignment=\"wts\")\n",
    "    # Calculate the euclidean distance between the supports\n",
    "    distance = ot.dist(support_x, support_y)\n",
    "    # Calculate beta\n",
    "    m = W_B.shape[0]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    # Calculate the transport matrix using optimal transport\n",
    "    transport_matrix = torch.from_numpy(ot.emd(alpha.numpy(), beta.numpy(), distance.detach().numpy())).float().reshape((n, m))\n",
    "    # Align model neurons\n",
    "    aligned_model = torch.matmul(torch.diag(1 / beta), torch.matmul(transport_matrix.T, aligned_W))\n",
    "    # Get the weights at layer \"idx\" from the second model\n",
    "    fused = (aligned_model + W_B) / 2 \n",
    "    return  fused, transport_matrix, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_multihead(nameA, nameB, weightA, weightB, transport_matrix, beta, head):\n",
    "    support_y = weightB\n",
    "    support_x = weightA\n",
    "    # Get the weights at layer \"idx\" from the first model\n",
    "    W_A = weightA\n",
    "    W_B = weightB\n",
    "    # Initialize the fused model and transport matrix\n",
    "    fused = torch.empty(W_B.shape)\n",
    "    transport_matrix_new = torch.zeros((weightA.shape[0], weightB.shape[0]))\n",
    "    stride = weightB.shape[0] // head\n",
    "    for i in range(0, weightB.shape[0], stride):\n",
    "        # Align the weights from the first model\n",
    "        aligned_W = torch.matmul(W_A[i:i+stride, :], torch.matmul(transport_matrix, torch.diag(1 / beta)))\n",
    "        # Get the X-Support\n",
    "        n = W_A.shape[0] // head\n",
    "        alpha = torch.ones(n) * (1/n)\n",
    "        # Calculate the euclidean distance between the supports\n",
    "        distance = ot.dist(support_x[i:i+stride, :], support_y[i:i+stride, :])\n",
    "        # Calculate beta\n",
    "        m = W_B.shape[0] // head\n",
    "        beta_new = torch.ones(m) * (1/m)\n",
    "        # Calculate the transport matrix using optimal transport\n",
    "        transport_matrix_new[i:i+stride, i:i+stride] = torch.from_numpy(ot.emd(alpha.numpy(), beta_new.numpy(), distance.detach().numpy())).float().reshape((n, m))\n",
    "        # Align model neurons\n",
    "        aligned_model = torch.matmul(torch.diag(1 / beta_new), torch.matmul(transport_matrix_new[i:i+stride, i:i+stride].T, aligned_W))\n",
    "        # Get the weights at layer \"idx\" from the second model\n",
    "        fused[i:i+stride, :] = (aligned_model + W_B[i:i+stride, :]) / 2 \n",
    "    return  fused, transport_matrix_new, beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_crossmultihead(nameA, nameB, weightA, weightB, transport_matrix, beta, head):\n",
    "    W_A_head = weightA.view(head, -1)\n",
    "    W_B_head = weightB.view(head, -1)\n",
    "    \n",
    "    m = W_B_head.shape[1]\n",
    "    beta_head = torch.ones(m) * (1/m)\n",
    "    transport_matrix_head = torch.matmul(torch.diag(beta_head), torch.eye(m))\n",
    "\n",
    "    support_y = getSupport(modelB, train_iter, nameB, alignment=\"wts\")\n",
    "    support_x = getSupport(modelA, train_iter, nameA, alignment=\"wts\")\n",
    "\n",
    "    aligned_W = torch.matmul(W_A_head, torch.matmul(transport_matrix_head, torch.diag(1 / beta_head)))\n",
    "\n",
    "    dist_head = ot.dist(support_x.view(head, -1), support_y.view(head, -1))\n",
    "\n",
    "    n = W_A_head.shape[0]\n",
    "    alpha_head = torch.ones(n) * (1/n)\n",
    "\n",
    "    m = W_B_head.shape[0]\n",
    "    beta_head = torch.ones(m) * (1/m)\n",
    "\n",
    "    transport_matrix_new = torch.from_numpy(ot.emd(alpha_head.numpy(), beta_head.numpy(), dist_head.detach().numpy())).float().reshape((n, m))\n",
    "\n",
    "    aligned_W_A = torch.matmul(torch.diag(1 / beta_head), torch.matmul(transport_matrix_new.T, aligned_W))\n",
    "    aligned_W_A = aligned_W_A.view(weightA.shape)\n",
    "    return fusion_multihead(nameA, nameB, aligned_W_A, weightB, transport_matrix, beta, head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion via Optimal Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusedModel = TransformerClassifier(src_pad_idx = pad_idx,\n",
    "                              embedding = embedding,\n",
    "                              enc_voc_size = voc_size,\n",
    "                              max_len = 256,\n",
    "                              d_model = 16,\n",
    "                              ffn_hidden = 32,\n",
    "                              n_head = 4,\n",
    "                              n_layers = 1,\n",
    "                              drop_prob = 0.2,\n",
    "                              device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    a = trial.suggest_float('a', 0, 1)\n",
    "    \n",
    "    # Create the fused weights matrix\n",
    "    W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "    # Initialize the algorithm\n",
    "    m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "\n",
    "    # Fusion via Optimal Transport\n",
    "    for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "        if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "            W_fusion[nameA] = weightA\n",
    "        else:\n",
    "            if \"weight\" in nameA:\n",
    "                if \"encoder\" in nameA:\n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        W_fusion[nameA], transport_matrix_triplet, _ = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "                    else:\n",
    "                        W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            elif \"bias\" in nameA:\n",
    "                if \"encoder\" in nameA: \n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                    else:\n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "\n",
    "    # Assign the weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in fusedModel.named_parameters():\n",
    "            param.data = torch.nn.Parameter(W_fusion[name])\n",
    "\n",
    "    # Validate the fused model\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-13 21:07:44,479]\u001b[0m A new study created in memory with name: no-name-669342b6-1f6a-4bd4-9d09-1ce40a3162ef\u001b[0m\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "\u001b[32m[I 2023-01-13 21:07:59,911]\u001b[0m Trial 0 finished with value: 0.5420436362425486 and parameters: {'a': 0.6333702342489141}. Best is trial 0 with value: 0.5420436362425486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:08:11,267]\u001b[0m Trial 1 finished with value: 0.6559023757775625 and parameters: {'a': 0.49535508407934137}. Best is trial 0 with value: 0.5420436362425486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:08:21,857]\u001b[0m Trial 2 finished with value: 0.5407184561093649 and parameters: {'a': 0.8508368252101267}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:08:32,750]\u001b[0m Trial 3 finished with value: 0.951775461435318 and parameters: {'a': 0.3099701577200987}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:08:43,327]\u001b[0m Trial 4 finished with value: 1.4170576731363933 and parameters: {'a': 0.1261287000621798}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:08:53,722]\u001b[0m Trial 5 finished with value: 1.7645795742670696 and parameters: {'a': 0.012004969938122056}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:09:04,443]\u001b[0m Trial 6 finished with value: 0.6917352875073751 and parameters: {'a': 0.4655669082975443}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:09:14,964]\u001b[0m Trial 7 finished with value: 1.4487883845965068 and parameters: {'a': 0.11572615842576106}. Best is trial 2 with value: 0.5407184561093649.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:09:25,595]\u001b[0m Trial 8 finished with value: 0.5189869354168574 and parameters: {'a': 0.7505934275341793}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:09:36,274]\u001b[0m Trial 9 finished with value: 0.5571078956127167 and parameters: {'a': 0.8868976410897519}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:09:49,642]\u001b[0m Trial 10 finished with value: 0.520423948764801 and parameters: {'a': 0.7006433710591432}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:10:05,462]\u001b[0m Trial 11 finished with value: 0.5247557709614435 and parameters: {'a': 0.6753036458522765}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:10:17,753]\u001b[0m Trial 12 finished with value: 0.523302361369133 and parameters: {'a': 0.6913251538638264}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:10:29,719]\u001b[0m Trial 13 finished with value: 0.5219479153553644 and parameters: {'a': 0.7932629151496415}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:10:42,437]\u001b[0m Trial 14 finished with value: 0.5803532501061758 and parameters: {'a': 0.9286380619565773}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:10:55,633]\u001b[0m Trial 15 finished with value: 0.8182752827803293 and parameters: {'a': 0.38055581814496975}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:11:08,674]\u001b[0m Trial 16 finished with value: 0.5492953658103943 and parameters: {'a': 0.6198256226877162}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:11:25,368]\u001b[0m Trial 17 finished with value: 0.6217202842235565 and parameters: {'a': 0.9823515045456317}. Best is trial 8 with value: 0.5189869354168574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:11:38,789]\u001b[0m Trial 18 finished with value: 0.518362487355868 and parameters: {'a': 0.7650230452640134}. Best is trial 18 with value: 0.518362487355868.\u001b[0m\n",
      "\u001b[32m[I 2023-01-13 21:11:52,222]\u001b[0m Trial 19 finished with value: 0.5204267303148905 and parameters: {'a': 0.7852846600878715}. Best is trial 18 with value: 0.518362487355868.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.5187215258677801\n",
      "Validation Accuracy:      0.7743644907994924\n"
     ]
    }
   ],
   "source": [
    "a = study.best_params[\"a\"]\n",
    "\n",
    "# Create the fused weights matrix\n",
    "W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "# Initialize the algorithm\n",
    "m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "beta = torch.ones(m) * (1/m)\n",
    "transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "# Fusion via Optimal Transport\n",
    "for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "    if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "        W_fusion[nameA] = weightA\n",
    "    else:\n",
    "        if \"weight\" in nameA:\n",
    "            if \"encoder\" in nameA:\n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    W_fusion[nameA], transport_matrix_triplet, _ = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "                else:\n",
    "                    W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        elif \"bias\" in nameA:\n",
    "            if \"encoder\" in nameA: \n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        else:\n",
    "            W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "# Assign the weights\n",
    "with torch.no_grad():\n",
    "    for name, param in fusedModel.named_parameters():\n",
    "        param.data = torch.nn.Parameter(W_fusion[name])\n",
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    a = trial.suggest_float('a', 0, 1)\n",
    "    \n",
    "    # Create the fused weights matrix\n",
    "    W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "    # Initialize the algorithm\n",
    "    m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "\n",
    "    # Fusion via Optimal Transport\n",
    "    for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "        if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "            W_fusion[nameA] = weightA\n",
    "        else:\n",
    "            if \"weight\" in nameA:\n",
    "                if \"encoder\" in nameA:\n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        transport_matrix_triplet = torch.empty((weightA.shape[0], weightB.shape[0]))\n",
    "                        W_fusion[nameA], transport_matrix_triplet, _ = fusion_multihead(nameA, nameB, weightA, weightB, transport_matrix, beta, fusedModel.n_head)\n",
    "                    else:\n",
    "                        W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            elif \"bias\" in nameA:\n",
    "                if \"encoder\" in nameA: \n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                    else:\n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "\n",
    "    # Assign the weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in fusedModel.named_parameters():\n",
    "            param.data = torch.nn.Parameter(W_fusion[name])\n",
    "\n",
    "    # Validate the fused model\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-06 16:21:48,102]\u001b[0m A new study created in memory with name: no-name-0d21f95c-1e34-4fc0-94d6-243a6466ca03\u001b[0m\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_14272/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "\u001b[32m[I 2023-01-06 16:22:02,207]\u001b[0m Trial 0 finished with value: 0.8599685231844584 and parameters: {'a': 0.8665591941348233}. Best is trial 0 with value: 0.8599685231844584.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:22:14,004]\u001b[0m Trial 1 finished with value: 0.8048130472501119 and parameters: {'a': 0.6766745617227093}. Best is trial 1 with value: 0.8048130472501119.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:22:26,913]\u001b[0m Trial 2 finished with value: 0.7487293680508932 and parameters: {'a': 0.2826387952577796}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:22:39,839]\u001b[0m Trial 3 finished with value: 0.8281092643737793 and parameters: {'a': 0.7457374865907416}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:22:53,143]\u001b[0m Trial 4 finished with value: 0.8842010299364725 and parameters: {'a': 0.9434237615010066}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:23:06,380]\u001b[0m Trial 5 finished with value: 0.8606274326642355 and parameters: {'a': 0.8617796130116399}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:23:21,537]\u001b[0m Trial 6 finished with value: 0.7822549740473429 and parameters: {'a': 0.5489187911546061}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:23:35,846]\u001b[0m Trial 7 finished with value: 0.7740474541982015 and parameters: {'a': 0.46749699655488686}. Best is trial 2 with value: 0.7487293680508932.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:23:49,635]\u001b[0m Trial 8 finished with value: 0.7456660469373068 and parameters: {'a': 0.1637180125571388}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:24:03,574]\u001b[0m Trial 9 finished with value: 0.7998786270618439 and parameters: {'a': 0.6227312884265573}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:24:17,442]\u001b[0m Trial 10 finished with value: 0.7479863166809082 and parameters: {'a': 0.019935254534460684}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:24:33,072]\u001b[0m Trial 11 finished with value: 0.7460979421933492 and parameters: {'a': 0.0014271244055247857}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:24:48,360]\u001b[0m Trial 12 finished with value: 0.7523863613605499 and parameters: {'a': 0.008226259089595928}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:25:03,881]\u001b[0m Trial 13 finished with value: 0.7504371404647827 and parameters: {'a': 0.1983630143126448}. Best is trial 8 with value: 0.7456660469373068.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:25:18,231]\u001b[0m Trial 14 finished with value: 0.7449023127555847 and parameters: {'a': 0.19544139458550663}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:25:32,924]\u001b[0m Trial 15 finished with value: 0.7502321700255076 and parameters: {'a': 0.31661610043568944}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:25:48,943]\u001b[0m Trial 16 finished with value: 0.7463810642560323 and parameters: {'a': 0.16533106364927197}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:26:06,367]\u001b[0m Trial 17 finished with value: 0.7638599375883738 and parameters: {'a': 0.42114137085348036}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:26:20,351]\u001b[0m Trial 18 finished with value: 0.7459341684977213 and parameters: {'a': 0.12280119416990537}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:26:32,301]\u001b[0m Trial 19 finished with value: 0.7564234733581543 and parameters: {'a': 0.3501712731728266}. Best is trial 14 with value: 0.7449023127555847.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_14272/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.7464419007301331\n",
      "Validation Accuracy:      0.7104021256345178\n"
     ]
    }
   ],
   "source": [
    "a = study.best_params[\"a\"]\n",
    "\n",
    "# Create the fused weights matrix\n",
    "W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "# Initialize the algorithm\n",
    "m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "beta = torch.ones(m) * (1/m)\n",
    "transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "# Fusion via Optimal Transport\n",
    "for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "    if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "        W_fusion[nameA] = weightA\n",
    "    else:\n",
    "        if \"weight\" in nameA:\n",
    "            if \"encoder\" in nameA:\n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    transport_matrix_triplet = torch.empty((weightA.shape[0], weightB.shape[0]))\n",
    "                    W_fusion[nameA], transport_matrix_triplet, _ = fusion_multihead(nameA, nameB, weightA, weightB, transport_matrix, beta, fusedModel.n_head)\n",
    "                else:\n",
    "                    W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        elif \"bias\" in nameA:\n",
    "            if \"encoder\" in nameA: \n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        else:\n",
    "            W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "# Assign the weights\n",
    "with torch.no_grad():\n",
    "    for name, param in fusedModel.named_parameters():\n",
    "        param.data = torch.nn.Parameter(W_fusion[name])\n",
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    a = trial.suggest_float('a', 0, 1)\n",
    "    \n",
    "    # Create the fused weights matrix\n",
    "    W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "    # Initialize the algorithm\n",
    "    m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "    beta = torch.ones(m) * (1/m)\n",
    "    transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "\n",
    "    # Fusion via Optimal Transport\n",
    "    for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "        if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "            W_fusion[nameA] = weightA\n",
    "        else:\n",
    "            if \"weight\" in nameA:\n",
    "                if \"encoder\" in nameA:\n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        transport_matrix_triplet = torch.empty((weightA.shape[0], weightB.shape[0]))\n",
    "                        W_fusion[nameA], transport_matrix_triplet, _ = fusion_crossmultihead(nameA, nameB, weightA, weightB, transport_matrix, beta, fusedModel.n_head)\n",
    "                    else:\n",
    "                        W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            elif \"bias\" in nameA:\n",
    "                if \"encoder\" in nameA: \n",
    "                    if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                    else:\n",
    "                        m = weightB.shape[0]\n",
    "                        beta_bias = torch.ones(m) * (1/m)\n",
    "                        W_A_bias = weightA.reshape(m, 1)\n",
    "                        aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                        aligned_bias = aligned_bias.reshape(m)\n",
    "                        W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "\n",
    "    # Assign the weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in fusedModel.named_parameters():\n",
    "            param.data = torch.nn.Parameter(W_fusion[name])\n",
    "\n",
    "    # Validate the fused model\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-06 16:36:56,804]\u001b[0m A new study created in memory with name: no-name-0368857a-7dd6-474c-9651-e6538bbb740f\u001b[0m\n",
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_14272/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "\u001b[32m[I 2023-01-06 16:37:16,300]\u001b[0m Trial 0 finished with value: 0.5930518607298533 and parameters: {'a': 0.3403607014689404}. Best is trial 0 with value: 0.5930518607298533.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:37:32,117]\u001b[0m Trial 1 finished with value: 0.6348852316538492 and parameters: {'a': 0.1764145706442095}. Best is trial 0 with value: 0.5930518607298533.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:37:46,683]\u001b[0m Trial 2 finished with value: 0.5379282633463541 and parameters: {'a': 0.9853141746449238}. Best is trial 2 with value: 0.5379282633463541.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:38:00,214]\u001b[0m Trial 3 finished with value: 0.5338148126999537 and parameters: {'a': 0.8956041870452112}. Best is trial 3 with value: 0.5338148126999537.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:38:13,494]\u001b[0m Trial 4 finished with value: 0.6243416468302408 and parameters: {'a': 0.21864719415526424}. Best is trial 3 with value: 0.5338148126999537.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:38:26,761]\u001b[0m Trial 5 finished with value: 0.5939013262589773 and parameters: {'a': 0.3369533029611853}. Best is trial 3 with value: 0.5338148126999537.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:38:40,072]\u001b[0m Trial 6 finished with value: 0.5520778596401215 and parameters: {'a': 0.5592674245395551}. Best is trial 3 with value: 0.5338148126999537.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:38:53,681]\u001b[0m Trial 7 finished with value: 0.5332629382610321 and parameters: {'a': 0.8127444996205082}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:39:07,277]\u001b[0m Trial 8 finished with value: 0.578099936246872 and parameters: {'a': 0.4105112088173951}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:39:20,746]\u001b[0m Trial 9 finished with value: 0.670069565375646 and parameters: {'a': 0.08179286393254359}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:39:34,399]\u001b[0m Trial 10 finished with value: 0.5386752188205719 and parameters: {'a': 0.7020243852439705}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:39:46,947]\u001b[0m Trial 11 finished with value: 0.5353521207968394 and parameters: {'a': 0.9365759799516249}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:39:59,483]\u001b[0m Trial 12 finished with value: 0.5363061527411143 and parameters: {'a': 0.7873387607909782}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:40:12,112]\u001b[0m Trial 13 finished with value: 0.5343887408574423 and parameters: {'a': 0.7851012242808992}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:40:24,759]\u001b[0m Trial 14 finished with value: 0.5452460547288259 and parameters: {'a': 0.6254267222192637}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:40:37,455]\u001b[0m Trial 15 finished with value: 0.5343262255191803 and parameters: {'a': 0.8518871942534388}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:40:50,497]\u001b[0m Trial 16 finished with value: 0.5426290929317474 and parameters: {'a': 0.6731003490473745}. Best is trial 7 with value: 0.5332629382610321.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:41:06,552]\u001b[0m Trial 17 finished with value: 0.5331501364707947 and parameters: {'a': 0.8778319403441719}. Best is trial 17 with value: 0.5331501364707947.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:41:22,858]\u001b[0m Trial 18 finished with value: 0.5603148639202118 and parameters: {'a': 0.5086104387756636}. Best is trial 17 with value: 0.5331501364707947.\u001b[0m\n",
      "\u001b[32m[I 2023-01-06 16:41:35,596]\u001b[0m Trial 19 finished with value: 0.535745436946551 and parameters: {'a': 0.755293597595076}. Best is trial 17 with value: 0.5331501364707947.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_14272/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.5346730997165045\n",
      "Validation Accuracy:      0.7670724936548222\n"
     ]
    }
   ],
   "source": [
    "a = study.best_params[\"a\"]\n",
    "\n",
    "# Create the fused weights matrix\n",
    "W_fusion = dict.fromkeys(list(modelA.state_dict().keys()))\n",
    "# Initialize the algorithm\n",
    "m = list(modelB.state_dict().items())[1][1].shape[1]\n",
    "beta = torch.ones(m) * (1/m)\n",
    "transport_matrix = torch.matmul(torch.diag(beta), torch.eye(m))\n",
    "# Fusion via Optimal Transport\n",
    "for (nameA, weightA), (nameB, weightB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
    "    if nameA == \"encoder.emb.tok_emb.embedding.weight\":\n",
    "        W_fusion[nameA] = weightA\n",
    "    else:\n",
    "        if \"weight\" in nameA:\n",
    "            if \"encoder\" in nameA:\n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    transport_matrix_triplet = torch.empty((weightA.shape[0], weightB.shape[0]))\n",
    "                    W_fusion[nameA], transport_matrix_triplet, _ = fusion_crossmultihead(nameA, nameB, weightA, weightB, transport_matrix, beta, fusedModel.n_head)\n",
    "                else:\n",
    "                    W_fusion[nameA], transport_matrix, beta = fusion(nameA, nameB, weightA, weightB, transport_matrix, beta)\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        elif \"bias\" in nameA:\n",
    "            if \"encoder\" in nameA: \n",
    "                if \"concat\" not in nameA and \"linear\" not in nameA: \n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix_triplet.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "                else:\n",
    "                    m = weightB.shape[0]\n",
    "                    beta_bias = torch.ones(m) * (1/m)\n",
    "                    W_A_bias = weightA.reshape(m, 1)\n",
    "                    aligned_bias = torch.matmul(torch.diag(1 / beta_bias), torch.matmul(transport_matrix.T, W_A_bias))\n",
    "                    aligned_bias = aligned_bias.reshape(m)\n",
    "                    W_fusion[nameA] = (aligned_bias + weightB) / 2\n",
    "            else:\n",
    "                W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "        else:\n",
    "            W_fusion[nameA] = a * weightA + (1-a) * weightB\n",
    "# Assign the weights\n",
    "with torch.no_grad():\n",
    "    for name, param in fusedModel.named_parameters():\n",
    "        param.data = torch.nn.Parameter(W_fusion[name])\n",
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibrate the normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1843898165.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 47/47 [02:16<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(fusedModel.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "clip = 1\n",
    "\n",
    "fusedModel.train()\n",
    "for name, param in fusedModel.named_parameters():\n",
    "    if \"weight\" in name or \"bias\" in name:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "for i, batch in enumerate(tqdm(train_iter)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = fusedModel(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(fusedModel.parameters(), clip)\n",
    "            optimizer.step() # optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.501120870312055\n",
      "Validation Accuracy:      0.7807113208544839\n"
     ]
    }
   ],
   "source": [
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in fusedModel.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/2985598703.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
      "100%|██████████| 47/47 [02:00<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(fusedModel.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "clip = 1\n",
    "\n",
    "fusedModel.train()\n",
    "for name, param in fusedModel.named_parameters():\n",
    "    if \"encoder\" in name:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "for i, batch in enumerate(tqdm(train_iter)):\n",
    "            src = batch[0] # X\n",
    "            trg = batch[1] # y\n",
    "            src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            output = fusedModel(src) # predict\n",
    "            y_pred = torch.argmax(output, dim=-1) # logits -> labels\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg.to(torch.int64)\n",
    "            loss = criterion(output_reshape, trg) # calculate loss\n",
    "            agreements = torch.eq(y_pred, trg)\n",
    "            accuracy = torch.mean(agreements.double()) # calculate accuracy\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(fusedModel.parameters(), clip)\n",
    "            optimizer.step() # optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:      0.46724362671375275\n",
      "Validation Accuracy:      0.809258010786802\n"
     ]
    }
   ],
   "source": [
    "# Validate the fused model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_loss, val_acc = validation(fusedModel, valid_iter, criterion, device)\n",
    "print(\"Validation Loss:     \", val_loss)\n",
    "print(\"Validation Accuracy:     \", val_acc.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atace\\AppData\\Local\\Temp/ipykernel_10896/1431534122.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src, trg = torch.tensor(src).to(device), torch.tensor(trg).to(device) # put to cpu/gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Fused Model:      0.4721055825551351\n",
      "Validation Accuracy Fused Model:      0.7976611946202531\n",
      "Validation Loss Model A:      0.44085009892781574\n",
      "Validation Accuracy Model A:      0.8333720661919831\n",
      "Validation Loss Model B:      0.43376828730106354\n",
      "Validation Accuracy Model B:      0.8321054193037974\n"
     ]
    }
   ],
   "source": [
    "# Test the models\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_fused, test_acc_fused = validation(fusedModel, test_iter, criterion, device)\n",
    "test_loss_A, test_acc_A = validation(modelA, test_iter, criterion, device)\n",
    "test_loss_B, test_acc_B = validation(modelB, test_iter, criterion, device)\n",
    "\n",
    "print(\"Validation Loss Fused Model:     \", test_loss_fused)\n",
    "print(\"Validation Accuracy Fused Model:     \", test_acc_fused.item())\n",
    "print(\"Validation Loss Model A:     \", test_loss_A)\n",
    "print(\"Validation Accuracy Model A:     \", test_acc_A.item())\n",
    "print(\"Validation Loss Model B:     \", test_loss_B)\n",
    "print(\"Validation Accuracy Model B:     \", test_acc_B.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1744cd9dc0832a8d503a2c77e6bee76d4493b3bf33a738cf38afd0bb2e60262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
